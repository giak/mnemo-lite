# POC #1: Redis Integration - Deep Analysis

---
type: RESEARCH
status: active
created: 2025-01-20
updated: 2025-01-20
lifecycle: temporary
supersedes: []
superseded_by: null
related:
  - 01_ARCHITECTURE_DECISIONS/ADR-001_cache_strategy.md
  - 03_EPICS/EPIC-10_PERFORMANCE_CACHING.md
contributors: [Human, Claude]
feeds_into: ADR-001_cache_strategy.md
---

**Version**: 1.0
**Date**: 2025-01-20
**Purpose**: Ultra-deep brainstorm on POC #1 to validate Redis integration hypotheses before implementation
**Status**: üü° Active Research
**Timeline**: 4 heures (estimation)

---

## üìã Executive Summary

**Probl√®me**: ADR-001 recommande Redis Standard pour L2 cache, mais **ZERO validation pratique** existe.

**Hypoth√®ses non valid√©es**:
- Redis 7.x compatible avec `redis.asyncio` (Python async client)
- Pub/Sub propagation <100ms pour multi-instance cache invalidation
- Latency <5ms pour get/set operations
- Connection pooling stable sous charge (100 req/s)
- Graceful degradation si Redis down (fallback PostgreSQL)

**Objectif POC**: Valider ces 5 hypoth√®ses avec **code ex√©cutable + m√©triques mesur√©es**.

**Risque si skip POC**: D√©couvrir incompatibilit√©s/limitations en production ‚Üí rollback co√ªteux.

---

## üéØ Objectifs du POC

### Objectif Principal
**Valider que Redis 7.x peut servir de L2 cache avec les caract√©ristiques promises dans ADR-001**.

### Objectifs Secondaires
1. **Performance**: Mesurer latency r√©elle (get/set/delete) vs 5ms target
2. **Multi-instance**: Valider Pub/Sub invalidation <100ms
3. **Resilience**: Tester graceful degradation (Redis down ‚Üí PostgreSQL)
4. **Scalability**: Connection pool stable √† 100 req/s
5. **Integration**: Async Python client compatible avec FastAPI lifecycle

### Non-Objectifs (Hors Scope)
- ‚ùå Redis Cluster (1 node suffit pour 1-3 API instances)
- ‚ùå Redis Sentinel (overkill pour scale actuelle)
- ‚ùå Persistence tuning (RDB/AOF defaults OK)
- ‚ùå Memory eviction policies (LRU default OK)

---

## üî¨ Contexte et D√©cisions √† Informer

### ADR-001: Cache Strategy
**D√©cision actuelle**: Redis Standard (15 ans, p√©renne) > Dragonfly (3 ans, 25√ó plus rapide)

**Points √† valider**:
- ‚úÖ Redis Standard latency acceptable (<5ms) ‚Üí sinon, reconsid√©rer Dragonfly
- ‚úÖ Async client stable ‚Üí sinon, sync client + threadpool
- ‚úÖ Pub/Sub fiable ‚Üí sinon, polling alternative
- ‚úÖ Graceful degradation ‚Üí sinon, risk d'outage total

**Impact si POC √©choue**:
- Dragonfly devient option #1 (performance > p√©rennit√© si Redis trop lent)
- Ou fallback sync client (impact: blocking I/O dans async app)

### EPIC-10: Performance Caching
**Claim actuel**: Triple-layer cache ‚Üí 65ms/file re-indexing (100√ó faster)

**Gaps identifi√©s**:
1. **Baseline manquante**: Aucun benchmark v2.0 document√© ‚Üí claim "100√ó" non prouvable
2. **Embedding cache unclear**: EPIC-10 cache `code_chunks` mais pas embeddings ‚Üí 80% du temps d'indexing non optimis√©?
3. **Graph construction non cach√©**: Seul traversal cach√© ‚Üí construction r√©p√©t√©e inutilement

**Ce POC doit valider**:
- Redis latency permet bien 100√ó gain (vs PostgreSQL cold)
- Connection pooling supporte load (sinon, bottleneck)

---

## üß™ Test Scenarios (Comprehensive)

### Scenario 1: Basic Operations
**Objectif**: Valider Redis 7.x + `redis.asyncio` fonctionnent ensemble.

**Tests**:
1. **Set/Get/Delete**:
   ```python
   await redis.set("key", "value", ex=60)  # TTL 60s
   result = await redis.get("key")
   await redis.delete("key")
   ```
   - **Success**: Operations r√©ussissent sans exception
   - **Metric**: Latency <5ms par op√©ration

2. **TTL expiration**:
   ```python
   await redis.set("key", "value", ex=1)  # 1 second TTL
   await asyncio.sleep(1.5)
   result = await redis.get("key")  # Should be None
   ```
   - **Success**: Key expire automatiquement apr√®s TTL
   - **Metric**: Expiration pr√©cise (¬±100ms)

3. **JSON serialization**:
   ```python
   data = {"code_chunk": {"id": "...", "embedding": [0.1, 0.2, ...]}}
   await redis.set("chunk:123", json.dumps(data))
   retrieved = json.loads(await redis.get("chunk:123"))
   ```
   - **Success**: Round-trip serialization sans perte
   - **Metric**: Serialization overhead <1ms

### Scenario 2: Pub/Sub Multi-Instance Invalidation
**Objectif**: Valider cache invalidation propagation entre 3 instances API.

**Setup**:
- 3 async Redis clients (simulent 3 API instances)
- Chaque client subscribe to `cache:invalidate` channel
- Client 1 publie invalidation message ‚Üí Clients 2 & 3 re√ßoivent

**Tests**:
1. **Basic Pub/Sub**:
   ```python
   # Instance 1: Publisher
   await redis1.publish("cache:invalidate", json.dumps({"key": "chunk:123"}))

   # Instances 2 & 3: Subscribers
   async for message in pubsub2.listen():
       if message['type'] == 'message':
           # Invalidate local cache
   ```
   - **Success**: Messages re√ßus par 100% subscribers
   - **Metric**: Propagation time <100ms (P99)

2. **High-frequency invalidation**:
   - Publier 100 invalidations/seconde
   - V√©rifier aucun message perdu
   - **Success**: 0% message loss
   - **Metric**: Latency stable m√™me sous charge

3. **Subscriber reconnection**:
   - Kill subscriber connection
   - Reconnect automatiquement
   - **Success**: Reconnection <1s
   - **Metric**: Aucun message perdu pendant reconnection

### Scenario 3: Connection Pooling Under Load
**Objectif**: Valider connection pool stable √† 100 req/s.

**Setup**:
- Connection pool: 10 connections (default)
- Simuler 100 req/s pendant 60 secondes
- Mesurer: latency, connection reuse, errors

**Tests**:
1. **Sustained load**:
   ```python
   async def simulate_request():
       await redis.get(f"key:{random.randint(1, 1000)}")

   # 100 req/s for 60s = 6000 requests
   await asyncio.gather(*[simulate_request() for _ in range(6000)])
   ```
   - **Success**: 0 connection errors
   - **Metric**: P50 <5ms, P99 <20ms

2. **Connection exhaustion**:
   - Pool size = 5 connections
   - Simuler 20 concurrent requests (4√ó pool size)
   - **Success**: Graceful queueing (no crash)
   - **Metric**: Max wait time <50ms

3. **Connection recovery**:
   - Restart Redis mid-test
   - **Success**: Auto-reconnect <2s
   - **Metric**: <1% failed requests

### Scenario 4: Graceful Degradation
**Objectif**: Valider fallback PostgreSQL si Redis down.

**Tests**:
1. **Redis unavailable at startup**:
   ```python
   try:
       redis = await create_redis_pool(...)
   except ConnectionError:
       logger.warning("Redis unavailable, using PostgreSQL only")
       redis = None
   ```
   - **Success**: API d√©marre quand m√™me
   - **Metric**: Startup time <5s

2. **Redis failure during runtime**:
   - API running with Redis
   - Stop Redis container
   - **Success**: API continue (PostgreSQL fallback)
   - **Metric**: No 500 errors, latency degradation acceptable (<500ms)

3. **Redis recovery during runtime**:
   - Redis red√©marre
   - **Success**: API reconnect automatiquement
   - **Metric**: Cache hit rate revient √† 80%+ dans 5 minutes

### Scenario 5: Memory and Eviction
**Objectif**: Valider behavior quand Redis atteint maxmemory.

**Tests**:
1. **LRU eviction**:
   - Set maxmemory = 100MB
   - Fill avec 150MB de data
   - **Success**: Least Recently Used keys √©vinc√©es
   - **Metric**: No OOM errors

2. **Eviction policy verification**:
   ```python
   info = await redis.info("memory")
   assert info['maxmemory_policy'] == 'allkeys-lru'
   ```
   - **Success**: Policy = allkeys-lru (pas noeviction)

---

## ‚úÖ Success Criteria (Measurable)

### Performance Criteria
| Metric | Target | Measurement Method |
|--------|--------|-------------------|
| **Get latency (P50)** | <5ms | `time.perf_counter()` over 1000 ops |
| **Get latency (P99)** | <20ms | 99th percentile of latencies |
| **Set latency (P50)** | <5ms | Same as get |
| **Pub/Sub propagation (P99)** | <100ms | Timestamp diff publisher ‚Üí subscriber |
| **Throughput** | 100 req/s sustained | 60s load test, 0 errors |
| **Connection pool stability** | 0 errors @ 100 req/s | Error count during load test |

### Functional Criteria
| Feature | Success Definition |
|---------|-------------------|
| **TTL expiration** | Keys expire within ¬±100ms of TTL |
| **JSON serialization** | Round-trip without data loss |
| **Pub/Sub delivery** | 100% message delivery to all subscribers |
| **Graceful degradation** | API functional when Redis down |
| **Auto-reconnect** | Reconnect <2s after Redis restart |

### Integration Criteria
| Aspect | Success Definition |
|--------|-------------------|
| **FastAPI lifecycle** | Redis pool created in `lifespan`, closed on shutdown |
| **Async compatibility** | No blocking I/O in async context |
| **Error handling** | No uncaught exceptions, graceful fallback |

---

## üõ†Ô∏è Implementation Approach

### Phase 1: Environment Setup (30 min)

**Docker Compose**:
```yaml
version: '3.8'
services:
  redis:
    image: redis:7.2-alpine
    ports:
      - "6379:6379"
    command: redis-server --maxmemory 100mb --maxmemory-policy allkeys-lru
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 5
```

**Python Dependencies**:
```bash
pip install redis[hiredis]==5.0.1  # Async client + faster parser
pip install pytest-asyncio pytest-benchmark
```

### Phase 2: Test Harness (1h)

**Structure**:
```
poc_redis/
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ test_redis_integration.py  # All scenarios
‚îú‚îÄ‚îÄ metrics.py                  # Latency/throughput measurement
‚îú‚îÄ‚îÄ README.md                   # Results documentation
‚îî‚îÄ‚îÄ requirements.txt
```

**Core Test Utilities**:
```python
# metrics.py
import time
import statistics
from typing import List

class LatencyMetrics:
    def __init__(self):
        self.latencies: List[float] = []

    async def measure(self, coro):
        start = time.perf_counter()
        result = await coro
        elapsed = (time.perf_counter() - start) * 1000  # ms
        self.latencies.append(elapsed)
        return result

    def report(self):
        return {
            "p50": statistics.median(self.latencies),
            "p99": statistics.quantiles(self.latencies, n=100)[98],
            "mean": statistics.mean(self.latencies),
            "count": len(self.latencies)
        }
```

### Phase 3: Execute Scenarios (2h)

**Test Execution Order**:
1. ‚úÖ Scenario 1: Basic Operations (validate setup works)
2. ‚úÖ Scenario 3: Connection Pooling (validate scalability)
3. ‚úÖ Scenario 2: Pub/Sub (validate multi-instance)
4. ‚úÖ Scenario 4: Graceful Degradation (validate resilience)
5. ‚úÖ Scenario 5: Memory/Eviction (validate resource limits)

**Metrics Collection**:
- JSON file: `results.json` with all latencies
- Markdown report: `RESULTS.md` (human-readable)
- Grafana dashboard (optional, si temps permet)

### Phase 4: Analysis & Decision (30 min)

**Decision Tree**:
```
All criteria met?
‚îú‚îÄ YES ‚Üí ‚úÖ Proceed with Redis Standard implementation
‚îú‚îÄ NO ‚Üí Performance issues?
    ‚îú‚îÄ YES ‚Üí üîÑ Consider Dragonfly (tradeoff p√©rennit√©)
    ‚îú‚îÄ NO ‚Üí Stability issues?
        ‚îú‚îÄ YES ‚Üí üîÑ Consider sync client + threadpool
        ‚îî‚îÄ NO ‚Üí üîÑ Consider alternative (Valkey, KeyDB)
```

---

## üö® Risks and Mitigations

### Risk 1: Redis 7.x incompatible avec redis.asyncio
**Probabilit√©**: Faible (redis-py 5.0+ stable)
**Impact**: √âlev√© (blocker pour async implementation)
**Mitigation**: Test basic operations first (Scenario 1), fallback sync client si n√©cessaire

### Risk 2: Pub/Sub latency >100ms
**Probabilit√©**: Moyenne (network latency variable)
**Impact**: Moyen (cache coherency delayed)
**Mitigation**:
- Augmenter timeout √† 500ms (acceptable pour cache invalidation)
- Ou polling alternative (check Redis key version)

### Risk 3: Connection pool instable sous charge
**Probabilit√©**: Faible (redis-py mature)
**Impact**: √âlev√© (bottleneck performance)
**Mitigation**:
- Tune pool size (default 10 ‚Üí 20 si n√©cessaire)
- Monitor connection reuse rate

### Risk 4: Graceful degradation √©choue
**Probabilit√©**: Moyenne (error handling non trivial)
**Impact**: Critique (outage total si Redis down)
**Mitigation**:
- Implement circuit breaker pattern
- Fallback PostgreSQL doit √™tre TOUJOURS test√©

### Risk 5: Memory eviction trop agressive
**Probabilit√©**: Faible (100MB sufficient pour cache)
**Impact**: Moyen (cache hit rate diminue)
**Mitigation**:
- Monitor eviction rate
- Augmenter maxmemory si >10% eviction rate

---

## üìä Expected Outcomes

### Scenario: Success (80% probability)
**R√©sultats attendus**:
- ‚úÖ Get/Set latency: 2-4ms (P50), 8-15ms (P99)
- ‚úÖ Pub/Sub propagation: 20-50ms (P99)
- ‚úÖ Throughput: 100 req/s stable
- ‚úÖ Graceful degradation: 0 errors when Redis down
- ‚úÖ Auto-reconnect: <1s

**D√©cision**:
- ‚úÖ **Proceed avec Redis Standard** (ADR-001 valid√©)
- ‚úÖ Update ADR-001 avec m√©triques mesur√©es
- ‚úÖ Move to POC #2 (LSP Pyright Query)

### Scenario: Partial Success (15% probability)
**R√©sultats possibles**:
- ‚ö†Ô∏è Latency: 8-12ms (P50) ‚Üí Acceptable mais pas optimal
- ‚ö†Ô∏è Pub/Sub: 150-200ms (P99) ‚Üí Lent mais fonctionnel

**D√©cision**:
- üîÑ **Benchmark Dragonfly** (POC #1b)
- üîÑ Si Dragonfly <2ms latency ‚Üí Reconsider p√©rennit√© vs performance tradeoff
- üîÑ Document tradeoffs dans ADR-001

### Scenario: Failure (5% probability)
**R√©sultats possibles**:
- ‚ùå Connection errors sous charge
- ‚ùå Pub/Sub message loss >1%
- ‚ùå Graceful degradation √©choue (500 errors)

**D√©cision**:
- üîÑ **Fallback sync client** + threadpool executor
- üîÑ Ou **Skip L2 cache** ‚Üí Dual-layer (L1 memory + L3 PostgreSQL)
- üîÑ Update ADR-001 avec nouvelle recommandation

---

## üîó Decision Points

### Decision Point 1: Redis Standard vs Dragonfly
**Crit√®re**: Latency <5ms (P50) avec Redis Standard?
- **YES** ‚Üí Proceed avec Redis Standard (p√©rennit√© wins)
- **NO** ‚Üí Benchmark Dragonfly (performance wins if <2ms)

### Decision Point 2: Async vs Sync Client
**Crit√®re**: redis.asyncio stable √† 100 req/s?
- **YES** ‚Üí Use async client (native FastAPI integration)
- **NO** ‚Üí Sync client + `asyncio.to_thread()` (perf hit acceptable)

### Decision Point 3: Pub/Sub vs Polling
**Crit√®re**: Pub/Sub propagation <100ms (P99)?
- **YES** ‚Üí Use Pub/Sub (optimal)
- **NO** ‚Üí Polling alternative (check key version every 5s)

### Decision Point 4: Triple-Layer vs Dual-Layer
**Crit√®re**: Graceful degradation works flawlessly?
- **YES** ‚Üí Triple-layer (L1 + L2 + L3)
- **NO** ‚Üí Dual-layer (L1 + L3, skip Redis)

---

## üìù Documentation des R√©sultats

### Template: RESULTS.md
```markdown
# POC #1: Redis Integration - Results

**Date**: YYYY-MM-DD
**Duration**: Xh
**Redis Version**: 7.2
**Python Client**: redis==5.0.1

## Test Environment
- OS: Ubuntu 22.04 / Docker
- CPU: [spec]
- Memory: [spec]
- Network: localhost (loopback)

## Scenario 1: Basic Operations
| Operation | P50 | P99 | Mean | Count | Target | Status |
|-----------|-----|-----|------|-------|--------|--------|
| GET       | Xms | Xms | Xms  | 1000  | <5ms   | ‚úÖ/‚ùå  |
| SET       | Xms | Xms | Xms  | 1000  | <5ms   | ‚úÖ/‚ùå  |
| DELETE    | Xms | Xms | Xms  | 1000  | <5ms   | ‚úÖ/‚ùå  |

## Scenario 2: Pub/Sub
| Metric | Value | Target | Status |
|--------|-------|--------|--------|
| Propagation (P99) | Xms | <100ms | ‚úÖ/‚ùå |
| Message Loss | X% | 0% | ‚úÖ/‚ùå |
| Reconnect Time | Xs | <1s | ‚úÖ/‚ùå |

## Scenario 3: Connection Pooling
| Metric | Value | Target | Status |
|--------|-------|--------|--------|
| Throughput | X req/s | 100 req/s | ‚úÖ/‚ùå |
| Errors | X | 0 | ‚úÖ/‚ùå |
| Latency (P99) | Xms | <20ms | ‚úÖ/‚ùå |

## Scenario 4: Graceful Degradation
| Test | Result | Status |
|------|--------|--------|
| Redis down @ startup | [description] | ‚úÖ/‚ùå |
| Redis down @ runtime | [description] | ‚úÖ/‚ùå |
| Redis recovery | [description] | ‚úÖ/‚ùå |

## Scenario 5: Memory/Eviction
| Test | Result | Status |
|------|--------|--------|
| LRU eviction | [description] | ‚úÖ/‚ùå |
| Eviction rate | X% | <10% | ‚úÖ/‚ùå |

## Decision
- [ ] ‚úÖ Proceed avec Redis Standard
- [ ] üîÑ Benchmark Dragonfly
- [ ] üîÑ Fallback sync client
- [ ] ‚ùå Skip L2 cache

## Next Steps
1. [action]
2. [action]
```

---

## üéØ Liens avec Autres POCs

### POC #2: LSP Pyright Query
**D√©pendance**: Aucune (peut √™tre parall√®le)
**Insight attendu**: Si LSP workspace mode 2-5√ó faster ‚Üí R√©duit besoin cache?

### POC #3: Triple-Cache Benchmark
**D√©pendance**: **BLOQU√â par POC #1** (need Redis validated)
**Insight attendu**: Mesure gain r√©el L1+L2+L3 vs L1+L3 (validate 100√ó claim)

### POC #4: Embedding Cache Strategy
**D√©pendance**: Partielle (Redis validated help, but not required)
**Insight attendu**: Si embeddings = 80% indexing time ‚Üí L2 cache MUST include embeddings, pas juste chunks

---

## üìö References

### Documentation
- Redis Python Client: https://redis-py.readthedocs.io/en/stable/
- Redis Pub/Sub: https://redis.io/docs/manual/pubsub/
- FastAPI Lifespan: https://fastapi.tiangolo.com/advanced/events/

### Code Examples
- ADR-001 (cache strategy): `01_ARCHITECTURE_DECISIONS/ADR-001_cache_strategy.md`
- EPIC-10 (implementation specs): `03_EPICS/EPIC-10_PERFORMANCE_CACHING.md`

### Related Issues
- Gap: Baseline benchmarks missing ‚Üí POC #3 must establish
- Gap: Embedding cache unclear ‚Üí POC #4 must clarify
- Gap: Dates obsol√®tes in MISSION_CONTROL ‚Üí Update apr√®s POC #1

---

## üöÄ Action Plan

### Immediate Next Steps (Next 4 hours)
1. **Setup environment** (30 min)
   - `docker-compose up -d`
   - Create `poc_redis/` directory structure
   - Install dependencies

2. **Implement test harness** (1h)
   - `metrics.py` (latency measurement)
   - `test_redis_integration.py` (5 scenarios)
   - `conftest.py` (pytest fixtures)

3. **Execute scenarios** (2h)
   - Run all 5 scenarios sequentially
   - Collect metrics in `results.json`
   - Document observations

4. **Analysis & decision** (30 min)
   - Generate `RESULTS.md` report
   - Make go/no-go decision
   - Update ADR-001 if validated

### Deliverables
- [ ] `poc_redis/` directory with all code
- [ ] `RESULTS.md` with measured metrics
- [ ] Decision: Proceed / Benchmark alternatives / Fallback
- [ ] Updated ADR-001 (if proceed)

---

## üí° Ultra-Thinking: Edge Cases & Deep Questions

### Edge Case 1: Clock Skew Between API Instances
**Scenario**: 3 API instances with clocks ¬±2s apart
**Impact**: TTL expiration inconsistent ‚Üí cache coherency issues
**Test**: Set key with TTL=5s, verify expiration within ¬±500ms across instances
**Mitigation**: Use Redis server time for TTL, not client time

### Edge Case 2: Network Partition During Pub/Sub
**Scenario**: Subscriber loses connection for 10s, then reconnects
**Impact**: Missed invalidation messages ‚Üí stale cache
**Test**: Disconnect subscriber, publish 10 invalidations, reconnect
**Mitigation**: Version-based cache validation (compare version on get)

### Edge Case 3: Thundering Herd on Cache Miss
**Scenario**: 100 concurrent requests miss cache ‚Üí 100 PostgreSQL queries
**Impact**: Database overload
**Test**: Flush Redis, send 100 concurrent identical requests
**Mitigation**: Request coalescing (first request fetches, others wait)

### Edge Case 4: Embedding Size Exceeds Redis String Limit
**Scenario**: Embedding = 768 floats √ó 4 bytes = 3KB per chunk √ó 1000 chunks = 3MB per cache entry
**Impact**: Redis string limit = 512MB (OK), but network latency increases
**Test**: Store 100 chunks with embeddings, measure get latency
**Mitigation**: Compress embeddings (zlib) if latency >10ms

### Deep Question 1: Cache Warming Strategy
**Question**: Should we pre-warm Redis on API startup?
**Tradeoffs**:
- PRO: Immediate cache hit on first request
- CON: Startup time +10s, complexity
**POC Test**: Measure startup time with/without warming (100 chunks)
**Decision Criteria**: If startup <5s with warming ‚Üí implement

### Deep Question 2: Cache Key Versioning
**Question**: Should cache keys include version (e.g., `v1:chunk:123`)?
**Tradeoffs**:
- PRO: Schema changes don't corrupt cache
- CON: Manual invalidation on version bump
**POC Test**: Not critical for POC, document for future
**Decision Criteria**: Implement if schema changes expected

### Deep Question 3: Redis Persistence (RDB vs AOF)
**Question**: Do we need persistence for cache?
**Tradeoffs**:
- NO persistence: Faster, cache is ephemeral (PostgreSQL = SSOT)
- RDB: Slower writes, cache survives restarts
- AOF: Slowest, maximum durability
**POC Test**: Measure write latency with RDB disabled vs enabled
**Decision Criteria**: If latency delta <1ms ‚Üí enable RDB (convenience)

---

## üß† Meta-Reflection: What This POC Teaches Us

### About Our Process
1. **Documentation ‚â† Validation**: ADR-001 √©tait excellent sur papier, mais **ZERO validation pratique** ‚Üí dangerous assumption
2. **Systematic Doubt works**: Ce POC existe parce que j'ai challeng√© l'ADR-001 ‚Üí d√©couvert gaps
3. **POCs prevent costly rollbacks**: 4h POC now >> 40h rollback later

### About Redis Integration
1. **Async client maturity**: redis-py 5.0+ devrait √™tre stable, mais **must verify** (breaking changes historiques)
2. **Pub/Sub subtleties**: Message delivery guarantees non √©videntes ‚Üí edge cases critiques
3. **Graceful degradation complexity**: Fallback PostgreSQL simple en th√©orie, **edge cases nombreux**

### About Performance Claims
1. **"100√ó faster" claim dangereux**: Sans baseline v2.0 + benchmark L1+L2+L3, claim non prouvable
2. **Embedding cache gap**: Si embeddings pas cach√©s ‚Üí 80% du gain potentiel perdu
3. **Redis latency ‚â† end-to-end latency**: Redis 2ms doesn't mean API response 2ms (serialization, network, etc.)

---

**STATUS**: üü¢ Ready to Execute
**BLOQU√â PAR**: Rien (can start immediately)
**BLOQUE**: POC #3 (Triple-Cache Benchmark)

**NEXT ACTION**: Create `poc_redis/` directory structure and begin implementation.
