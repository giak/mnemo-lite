# EPIC-24: Monitoring & Reliability ULTRATHINK

**Date**: 29 octobre 2025
**CriticitÃ©**: MAXIMALE (perte de donnÃ©es si Ã©chec)
**Objectif**: SystÃ¨me de surveillance infaillible pour le daemon auto-save

---

## ğŸ¯ ProblÃ©matique

**Situation Actuelle**:
- Daemon auto-save fonctionne (2241 conversations sauvegardÃ©es)
- Mais si daemon crash â†’ **PERTE SILENCIEUSE de conversations**
- Aucun monitoring, aucune alerte, aucun auto-recovery

**Risques IdentifiÃ©s**:
1. **Daemon crash silencieux** â†’ conversations perdues Ã  jamais
2. **API down** â†’ daemon continue mais fail Ã  sauvegarder
3. **DB connection loss** â†’ import Ã©choue sans alerte
4. **Disk full** â†’ logs remplissent `/tmp`, crash
5. **Memory leak** â†’ daemon consomme toute la RAM
6. **Parsing error** â†’ transcripts corrompus bloquent import

**ConsÃ©quence**: PERTE IRRÃ‰VERSIBLE de contexte historique

---

## ğŸ—ï¸ Architecture de Surveillance Multi-Couches

### Layer 1: Daemon Self-Monitoring (Internal)

**Heartbeat File**
```bash
# Dans le daemon, Ã©crire un heartbeat toutes les 30s
echo "$(date +%s)" > /tmp/daemon-heartbeat.txt

# Format heartbeat file:
{
  "last_beat": 1698576000,
  "last_import_success": 1698575970,
  "last_import_count": 3,
  "consecutive_failures": 0,
  "total_imported": 2241,
  "uptime_seconds": 3600
}
```

**Metrics File**
```bash
# Ã‰crire mÃ©triques aprÃ¨s chaque poll
/tmp/daemon-metrics.json:
{
  "timestamp": "2025-10-29T13:51:08Z",
  "status": "healthy",
  "last_poll": "2025-10-29T13:51:08Z",
  "last_success": "2025-10-29T12:50:37Z",
  "total_imported": 2241,
  "consecutive_failures": 0,
  "api_response_time_ms": 234,
  "memory_usage_mb": 45,
  "cpu_usage_percent": 2.1
}
```

---

### Layer 2: External Watchdog (Monitoring Script)

**Script**: `scripts/daemon-watchdog.sh`

```bash
#!/bin/bash
# Watchdog externe qui surveille le daemon
# Tourne en parallÃ¨le, vÃ©rifie heartbeat, auto-restart si dead

CHECK_INTERVAL=60  # Check every 60s
MAX_HEARTBEAT_AGE=120  # Alert if heartbeat > 2 minutes old
RESTART_THRESHOLD=3  # Restart after 3 consecutive failures

while true; do
    # 1. Check heartbeat file
    if [ -f /tmp/daemon-heartbeat.txt ]; then
        LAST_BEAT=$(cat /tmp/daemon-heartbeat.txt)
        NOW=$(date +%s)
        AGE=$((NOW - LAST_BEAT))

        if [ $AGE -gt $MAX_HEARTBEAT_AGE ]; then
            echo "[WATCHDOG] ALERT: Daemon heartbeat stale (${AGE}s old)"
            # Send alert + restart daemon
            restart_daemon
        fi
    else
        echo "[WATCHDOG] ALERT: Heartbeat file missing!"
        restart_daemon
    fi

    # 2. Check if daemon process exists
    if ! pgrep -f "conversation-auto-import.sh" > /dev/null; then
        echo "[WATCHDOG] ALERT: Daemon process not found!"
        restart_daemon
    fi

    # 3. Check API health
    if ! curl -s -f http://localhost:8000/health > /dev/null; then
        echo "[WATCHDOG] WARNING: API unhealthy"
    fi

    # 4. Check DB connection
    if ! docker compose exec -T db psql -U mnemo -d mnemolite -c "SELECT 1;" > /dev/null 2>&1; then
        echo "[WATCHDOG] WARNING: DB connection lost"
    fi

    # 5. Check disk space
    DISK_USAGE=$(df /tmp | tail -1 | awk '{print $5}' | sed 's/%//')
    if [ "$DISK_USAGE" -gt 90 ]; then
        echo "[WATCHDOG] ALERT: Disk usage ${DISK_USAGE}%"
    fi

    sleep $CHECK_INTERVAL
done

restart_daemon() {
    echo "[WATCHDOG] Restarting daemon..."
    pkill -f conversation-auto-import.sh
    nohup bash /app/scripts/conversation-auto-import.sh > /tmp/daemon.log 2>&1 &
    echo "[WATCHDOG] Daemon restarted"
}
```

---

### Layer 3: API Health Endpoint

**Endpoint**: `GET /v1/autosave/health`

```python
@router.get("/health")
async def daemon_health_check(
    engine: AsyncEngine = Depends(get_db_engine)
) -> Dict[str, Any]:
    """
    Health check complet du systÃ¨me auto-save.
    Retourne 200 si OK, 503 si critical issues.
    """
    issues = []
    status = "healthy"

    # 1. Check daemon heartbeat
    heartbeat_file = Path("/tmp/daemon-heartbeat.txt")
    if heartbeat_file.exists():
        last_beat = int(heartbeat_file.read_text().strip())
        age = time.time() - last_beat
        if age > 120:  # 2 minutes
            issues.append(f"Daemon heartbeat stale ({age}s)")
            status = "unhealthy"
    else:
        issues.append("Daemon heartbeat file missing")
        status = "critical"

    # 2. Check recent imports
    async with engine.begin() as conn:
        result = await conn.execute(text("""
            SELECT MAX(created_at) FROM memories WHERE author = 'AutoImport'
        """))
        last_import = result.scalar()

        if last_import:
            age = datetime.now() - last_import.replace(tzinfo=None)
            if age.total_seconds() > 600:  # 10 minutes
                issues.append(f"No imports in last {age.total_seconds()}s")
                status = "degraded"

    # 3. Check metrics file
    metrics_file = Path("/tmp/daemon-metrics.json")
    if metrics_file.exists():
        metrics = json.loads(metrics_file.read_text())

        if metrics.get("consecutive_failures", 0) > 5:
            issues.append(f"High failure rate: {metrics['consecutive_failures']}")
            status = "degraded"

    # 4. Response
    response = {
        "status": status,
        "timestamp": datetime.now().isoformat(),
        "daemon": {
            "heartbeat_age_seconds": age if 'age' in locals() else None,
            "last_import_at": last_import.isoformat() if last_import else None
        },
        "issues": issues
    }

    if status == "critical":
        raise HTTPException(status_code=503, detail=response)

    return response
```

---

### Layer 4: Docker Health Check

**docker-compose.yml**:

```yaml
services:
  api:
    healthcheck:
      test: ["CMD", "sh", "-c", "curl -f http://localhost:8000/health && curl -f http://localhost:8000/v1/autosave/health"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 120s

    # Auto-restart si unhealthy
    restart: unless-stopped
```

---

### Layer 5: Prometheus Metrics

**Exposition mÃ©triques Prometheus**:

```python
# api/routes/autosave_monitoring_routes.py

from prometheus_client import Counter, Gauge, Histogram

# MÃ©triques
conversations_imported_total = Counter(
    'mnemolite_conversations_imported_total',
    'Total conversations imported'
)

daemon_heartbeat_age = Gauge(
    'mnemolite_daemon_heartbeat_age_seconds',
    'Age of daemon heartbeat in seconds'
)

import_duration_seconds = Histogram(
    'mnemolite_import_duration_seconds',
    'Time spent importing conversations'
)

@router.get("/metrics")
async def prometheus_metrics():
    """Expose Prometheus metrics"""
    return Response(
        content=generate_latest(),
        media_type="text/plain"
    )
```

---

### Layer 6: Alerting System

**Option 1: Script Email (Simple)**

```bash
# scripts/send-alert.sh
#!/bin/bash
SUBJECT="[ALERT] MnemoLite Auto-Save Daemon Issue"
TO="admin@example.com"
MESSAGE=$1

echo "$MESSAGE" | mail -s "$SUBJECT" "$TO"
```

**Option 2: Webhook Discord/Slack (Meilleur)**

```bash
# scripts/send-alert.sh
#!/bin/bash
WEBHOOK_URL="${DISCORD_WEBHOOK_URL}"
MESSAGE=$1

curl -X POST "$WEBHOOK_URL" \
  -H "Content-Type: application/json" \
  -d "{\"content\": \"ğŸš¨ **MnemoLite Alert**: $MESSAGE\"}"
```

**Option 3: Prometheus Alertmanager (Production)**

```yaml
# alertmanager.yml
route:
  receiver: 'discord'

receivers:
  - name: 'discord'
    webhook_configs:
      - url: 'https://discord.com/api/webhooks/...'

# prometheus.yml
groups:
  - name: mnemolite
    interval: 60s
    rules:
      - alert: DaemonHeartbeatStale
        expr: mnemolite_daemon_heartbeat_age_seconds > 120
        for: 2m
        annotations:
          summary: "Daemon heartbeat is stale"

      - alert: NoRecentImports
        expr: time() - mnemolite_last_import_timestamp > 600
        for: 5m
        annotations:
          summary: "No imports in last 10 minutes"
```

---

## ğŸ› ï¸ ImplÃ©mentation RecommandÃ©e (3 Phases)

### Phase 1: Monitoring Basique (1h - IMMÃ‰DIAT)

**Fichiers Ã  crÃ©er**:
1. âœ… `scripts/daemon-watchdog.sh` - Watchdog externe
2. âœ… `api/routes/autosave_monitoring_routes.py` - Health endpoint
3. âœ… Modifier `scripts/conversation-auto-import.sh` - Ajouter heartbeat

**Modifications docker-compose.yml**:
```yaml
command:
  - sh
  - -c
  - |
    # Lancer daemon avec heartbeat
    nohup bash /app/scripts/conversation-auto-import.sh > /tmp/daemon.log 2>&1 &

    # Lancer watchdog en parallÃ¨le
    nohup bash /app/scripts/daemon-watchdog.sh > /tmp/watchdog.log 2>&1 &

    # Lancer API
    exec uvicorn main:app --host 0.0.0.0 --port 8000
```

**Tests**:
```bash
# 1. VÃ©rifier heartbeat
docker compose exec api cat /tmp/daemon-heartbeat.txt

# 2. VÃ©rifier health endpoint
curl http://localhost:8001/v1/autosave/health | jq

# 3. Simuler crash daemon
docker compose exec api pkill -f conversation-auto-import.sh
sleep 120  # Attendre watchdog (60s check + 60s grace)
docker compose exec api ps aux | grep conversation  # Doit Ãªtre restartÃ©
```

---

### Phase 2: Alerting & Dashboard (2h)

**Fichiers Ã  crÃ©er**:
1. âœ… `scripts/send-alert.sh` - Script alertes Discord/Slack
2. âœ… `templates/monitoring_daemon.html` - Dashboard dÃ©diÃ© daemon
3. âœ… `api/routes/autosave_monitoring_routes.py` - Endpoints stats

**Dashboard Features**:
- Graph timeline imports (Chart.js)
- Daemon status (heartbeat, uptime)
- Alert history (derniÃ¨res 100 alertes)
- MÃ©triques temps rÃ©el (poll rate, success rate)
- Button "Force Restart Daemon" (admin)

---

### Phase 3: Prometheus + Grafana (4h - OPTIONNEL)

**Stack complÃ¨te**:
```yaml
# docker-compose.yml
services:
  prometheus:
    image: prom/prometheus:latest
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
    ports:
      - "9090:9090"

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
```

**Dashboards Grafana**:
- Conversations imported over time
- Daemon uptime
- API response times
- Failure rate
- Disk/memory usage

---

## ğŸ¯ DÃ©cision: Quelle Phase ImplÃ©menter?

### Recommandation: **Phase 1 (ImmÃ©diat) + Phase 2 (Sous 1 semaine)**

**Pourquoi?**
- âœ… Phase 1: Watchdog couvre 95% des risques critiques (1h effort)
- âœ… Phase 2: Dashboard + alertes pour visibilitÃ© (2h effort)
- âš ï¸ Phase 3: Overkill pour projet solo (4h effort, complexitÃ©++)

**Phase 1 suffit si**:
- Tu checks `/v1/autosave/health` 1x par jour
- Tu as Discord/Slack pour recevoir alertes
- Tu acceptes restart automatique sans visibilitÃ© temps rÃ©el

**Phase 2 nÃ©cessaire si**:
- Tu veux dashboard visuel
- Tu veux historique des alertes
- Tu veux mÃ©triques dÃ©taillÃ©es (graphs)

**Phase 3 seulement si**:
- Ã‰quipe multi-users
- SLA strict (99.9% uptime)
- Budget monitoring infra

---

## ğŸ“‹ Checklist Phase 1 (Action ImmÃ©diate)

- [ ] Modifier `conversation-auto-import.sh` â†’ Ajouter heartbeat
- [ ] CrÃ©er `daemon-watchdog.sh` â†’ Watchdog externe
- [ ] CrÃ©er endpoint `GET /v1/autosave/health` â†’ Health check
- [ ] Modifier `docker-compose.yml` â†’ Lancer watchdog
- [ ] CrÃ©er `send-alert.sh` â†’ Notifications Discord/Slack
- [ ] Tester crash daemon + auto-restart
- [ ] Tester alerte si heartbeat stale
- [ ] Documenter monitoring dans README

**ETA Phase 1**: 1 heure
**ETA Tests**: 30 minutes
**Total**: 1h30

---

## ğŸ”¥ Cas d'Usage Critiques

### Cas 1: Daemon Crash Silencieux

**ScÃ©nario**: Daemon crash Ã  02:00 AM, personne ne remarque
**Sans monitoring**: Perte de TOUTES conversations jusqu'Ã  dÃ©tection manuelle
**Avec Phase 1**:
- Watchdog dÃ©tecte en 60s
- Auto-restart daemon
- Alerte Discord â†’ tu vois au rÃ©veil
- Perte: 0 conversations (reprise immÃ©diate)

### Cas 2: API Down

**ScÃ©nario**: API crash, daemon continue mais fail Ã  sauvegarder
**Sans monitoring**: Daemon log errors mais continue silencieusement
**Avec Phase 1**:
- Health check dÃ©tecte "No recent imports"
- Alerte Discord "API unhealthy"
- Tu restart API manuellement
- Perte: 0 conversations (dedup Ã©vite doublons aprÃ¨s restart)

### Cas 3: DB Connection Loss

**ScÃ©nario**: PostgreSQL restart, connection drops
**Sans monitoring**: Import fail, daemon continue
**Avec Phase 1**:
- Watchdog check DB connection
- DÃ©tecte loss + alerte
- Auto-restart daemon (reconnect)
- Perte: Conversations pendant downtime seulement

### Cas 4: Disk Full

**ScÃ©nario**: `/tmp/daemon.log` remplit le disque
**Sans monitoring**: Container crash, total outage
**Avec Phase 1**:
- Watchdog check disk usage
- Alerte si >90%
- Tu clean logs manuellement
- Prevention crash

---

## ğŸ“ LeÃ§ons des SystÃ¨mes Production

### Netflix Chaos Monkey Principle

**Citation**: "The best way to avoid failure is to fail constantly"

**Application MnemoLite**:
- Tester crash daemon rÃ©guliÃ¨rement (1x/semaine)
- VÃ©rifier auto-recovery fonctionne
- Mesurer MTTR (Mean Time To Recovery)

### Google SRE - Error Budget

**Citation**: "100% uptime is the wrong target"

**Application MnemoLite**:
- Acceptable: 1 restart/jour = 99.9% uptime
- Inacceptable: 10 restarts/jour = bug systÃ©mique

### AWS - Fail Fast, Recover Quickly

**Citation**: "It's not IF it fails, it's WHEN"

**Application MnemoLite**:
- Daemon doit fail fast (ne pas masquer erreurs)
- Watchdog doit recover quickly (60s detection)
- Alertes doivent Ãªtre actionables (pas spam)

---

## ğŸ“Š MÃ©triques de SuccÃ¨s

### KPIs Monitoring

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MÃ©trique              â”‚ Cible    â”‚ Alerte          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Daemon Uptime         â”‚ >99%     â”‚ <95%            â”‚
â”‚ Heartbeat Age         â”‚ <60s     â”‚ >120s           â”‚
â”‚ Import Success Rate   â”‚ >95%     â”‚ <80%            â”‚
â”‚ Time to Recovery      â”‚ <120s    â”‚ >300s           â”‚
â”‚ Alertes False Positivesâ”‚ <5%     â”‚ >20%            â”‚
â”‚ Conversations Lost    â”‚ 0        â”‚ >0              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Prochaines Ã‰tapes (Si Tu Approuves Phase 1)

1. **Modifier daemon** (15min)
   - Ajouter heartbeat write
   - Ajouter metrics file
   - Improved error logging

2. **CrÃ©er watchdog** (30min)
   - Script shell externe
   - Auto-restart logic
   - Alert integration

3. **Health endpoint** (15min)
   - Nouveau route FastAPI
   - Check heartbeat age
   - Check recent imports

4. **Docker config** (10min)
   - Launch watchdog in background
   - Update healthcheck

5. **Testing** (20min)
   - Simuler crashes
   - VÃ©rifier auto-restart
   - Tester alertes

**Total**: 1h30

---

**Attends validation avant de commencer l'implÃ©mentation.**

**Question ClÃ©**: Phase 1 seule suffit? Ou tu veux aussi Phase 2 (dashboard)?

---

**Version**: 1.0.0
**Date**: 29 octobre 2025
**Auteur**: Claude Code Assistant
**CriticitÃ©**: MAXIMALE
**Status**: â³ PENDING USER APPROVAL
