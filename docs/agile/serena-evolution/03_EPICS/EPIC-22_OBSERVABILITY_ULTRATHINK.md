# EPIC-22: Advanced Observability & Real-Time Monitoring - ULTRATHINK

**Date**: 2025-10-24
**Status**: ğŸ§  **BRAINSTORMING**
**PrioritÃ©**: P1 (Production-Critical)
**DÃ©pendances**: EPIC-08 (Cache L2) âœ…, EPIC-10 (Perf Validation) âœ…

---

## ğŸ“Š Ã‰tat des Lieux (Ce qui existe)

### âœ… Monitoring Existant

**Dashboard UI** (`/ui/monitoring`):
- âœ… KPI cards (events critical/warning/info, DB size)
- âœ… Timeline events (ECharts)
- âœ… Events critiques (24h)
- âœ… Distribution par sÃ©vÃ©ritÃ©/projet/catÃ©gorie
- âœ… Auto-refresh 30s

**API Endpoints Existants**:
```
âœ… /api/monitoring/status               â†’ System status
âœ… /api/monitoring/events/critical      â†’ Critical events (24h)
âœ… /api/monitoring/events/timeline      â†’ Timeline (24h/7d/30d)
âœ… /api/monitoring/events/distribution  â†’ Distribution by severity/project/category
âœ… /api/monitoring/metrics              â†’ Total events, DB size, cache hit ratio, connections

âœ… /performance/metrics                 â†’ App metrics (counters, timers, gauges)
âœ… /performance/cache/stats             â†’ Query cache + embedding cache stats
âœ… /performance/system                  â†’ CPU, memory, disk, process stats
âœ… /performance/database/pool           â†’ DB pool stats (connections)
âœ… /performance/database/slow-queries   â†’ Slow queries (pg_stat_statements)
âœ… /performance/database/indexes        â†’ Index usage stats
âœ… /performance/optimize/suggestions    â†’ Auto suggestions

âœ… /v1/cache/stats                      â†’ L1/L2 cascade cache stats
âœ… /v1/cache/flush                      â†’ Manual cache flush
âœ… /v1/cache/clear-all                  â†’ Clear all caches
```

**Forces**:
- âœ… Foundation solide (PostgreSQL, cache, system metrics)
- âœ… SCADA theme cohÃ©rent
- âœ… Zero dÃ©pendances externes (pas de Prometheus/Grafana)
- âœ… HTMX + ECharts dÃ©jÃ  intÃ©grÃ©s

---

## âŒ Ce Qui Manque (Gaps Critiques)

### 1. âŒ **Redis Monitoring DÃ©taillÃ©**

**ProblÃ¨me**: `/v1/cache/stats` retourne stats L1/L2, mais **pas de dÃ©tails Redis** :
- âŒ Memory usage Redis (used/max/percent)
- âŒ Hit rate par cache type (search_results, embeddings, graph_traversal)
- âŒ Keys count par type
- âŒ Evictions (LRU)
- âŒ Slow commands (SLOWLOG)
- âŒ Connections actives
- âŒ Average TTL par type

**Impact**: Impossible de diagnostiquer :
- "Pourquoi hit rate baisse ?" â†’ Pas de breakdown par type
- "Redis plein ?" â†’ Pas de vue memory
- "Evictions ?" â†’ Pas de metric

---

### 2. âŒ **API Performance Par Endpoint**

**ProblÃ¨me**: `/performance/metrics` agrÃ¨ge tout, **pas de dÃ©tails par endpoint** :
- âŒ Latency P50/P95/P99 par endpoint
- âŒ Throughput (req/s) par endpoint
- âŒ Error rate par endpoint
- âŒ Top erreurs avec stack traces
- âŒ Timeline latency (heatmap)

**Impact**: Impossible de rÃ©pondre Ã  :
- "Quel endpoint est lent ?"
- "Quelle erreur provoque les timeouts ?"
- "Latency normale pour /v1/code/search/hybrid ?"

---

### 3. âŒ **Logs en Temps RÃ©el (Streaming)**

**ProblÃ¨me**: Pas de visualisation logs dans l'UI :
- âŒ Stream logs temps rÃ©el (SSE)
- âŒ Filtres (level, service, search)
- âŒ Colorisation par level
- âŒ Export logs (CSV/JSON)

**Impact**: Pour dÃ©boguer, il faut :
1. SSH sur serveur
2. `docker logs mnemo-api | grep ERROR`
3. Pas d'historique filtrable

---

### 4. âŒ **Dashboard UnifiÃ© (One-Pager)**

**ProblÃ¨me**: Monitoring dispersÃ© :
- `/ui/monitoring` â†’ Events only
- `/performance/*` â†’ API calls only
- `/v1/cache/*` â†’ API calls only

**Besoin**: **Dashboard unifiÃ©** avec tout sur 1 page :
- API KPIs (latency, throughput)
- Redis KPIs (hit rate, memory)
- PostgreSQL KPIs (connections, slow queries)
- Logs stream
- System resources (CPU, RAM)

---

### 5. âŒ **Alerting & Notifications**

**ProblÃ¨me**: Aucune alerte automatique :
- âŒ Cache hit rate < 70% â†’ Warning
- âŒ Memory > 80% â†’ Critical
- âŒ Slow queries > 1000ms â†’ Warning
- âŒ Evictions > 100/h â†’ Info

**Impact**: RÃ©actif au lieu de proactif

---

### 6. âŒ **Request Tracing (Trace ID)**

**ProblÃ¨me**: Pas de trace_id cross-layers :
- RequÃªte API â†’ Service â†’ Repository â†’ PostgreSQL
- Impossible de suivre une requÃªte end-to-end

**Besoin**: Middleware trace_id + logs structlog avec trace_id

---

### 7. âŒ **Historical Trends (Comparaison Temporelle)**

**ProblÃ¨me**: Pas de comparaison "maintenant vs hier" :
- Latency P95 aujourd'hui: 120ms
- Latency P95 hier: 105ms â†’ +14.3% ğŸš¨

**Impact**: DÃ©tecter trends avant qu'ils ne deviennent critiques

---

## ğŸ¯ Proposition EPIC-22: Advanced Observability

### Vision

**"En 30 secondes, l'admin peut diagnostiquer n'importe quel problÃ¨me production depuis l'UI"**

**Features**:
1. **Dashboard UnifiÃ©** â†’ `/ui/monitoring/advanced` (one-pager)
2. **Redis Deep Dive** â†’ Breakdown par cache type, memory, evictions
3. **API Performance** â†’ Latency P50/P95/P99 par endpoint, heatmap
4. **Logs Streaming** â†’ SSE temps rÃ©el avec filtres
5. **Request Tracing** â†’ trace_id end-to-end
6. **Smart Alerting** â†’ Seuils configurables + notifications UI
7. **Historical Trends** â†’ Comparaison temporelle

---

## ğŸ—ï¸ Architecture EPIC-22 (Approche KISS)

### Principe: **EXTEND > REBUILD**

**Stack (Zero Nouvelle DÃ©pendance)**:
- Logs â†’ `structlog` (dÃ©jÃ  prÃ©sent) + SSE endpoint
- MÃ©triques â†’ Table PostgreSQL `metrics` (nouvelle)
- UI â†’ HTMX + ECharts (dÃ©jÃ  prÃ©sents)
- Temps rÃ©el â†’ Server-Sent Events (native browser)
- Cache â†’ Redis (dÃ©jÃ  prÃ©sent)

**Architecture**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ UI: /ui/monitoring/advanced                                     â”‚
â”‚ â”œâ”€ 4 KPI Cards (API, Redis, PostgreSQL, System)                â”‚
â”‚ â”œâ”€ 3 Line Charts (Latency, Hit Rate, Connections)              â”‚
â”‚ â”œâ”€ 1 Table (Slow Queries)                                      â”‚
â”‚ â””â”€ 1 Live Logs Stream (SSE)                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“ HTMX polling (5s) + SSE
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FastAPI Backend                                                 â”‚
â”‚                                                                 â”‚
â”‚ /api/monitoring/advanced/summary â†’ JSON                        â”‚
â”‚ /api/monitoring/redis/detailed â†’ JSON                          â”‚
â”‚ /api/monitoring/api/endpoints â†’ JSON                           â”‚
â”‚ /api/monitoring/logs/stream â†’ SSE                              â”‚
â”‚                                                                 â”‚
â”‚ Middleware: TraceIDMiddleware (inject trace_id)                â”‚
â”‚ Service: MetricsCollector (record metrics)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“                    â†“                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PostgreSQL       â”‚  â”‚ Redis            â”‚  â”‚ Logs             â”‚
â”‚                  â”‚  â”‚                  â”‚  â”‚                  â”‚
â”‚ Table: metrics   â”‚  â”‚ INFO commands    â”‚  â”‚ structlog        â”‚
â”‚ â”œâ”€ timestamp     â”‚  â”‚ â”œâ”€ memory        â”‚  â”‚ â”œâ”€ trace_id     â”‚
â”‚ â”œâ”€ metric_type   â”‚  â”‚ â”œâ”€ keys count    â”‚  â”‚ â”œâ”€ level        â”‚
â”‚ â”œâ”€ metric_name   â”‚  â”‚ â””â”€ slowlog       â”‚  â”‚ â””â”€ endpoint     â”‚
â”‚ â”œâ”€ value         â”‚  â”‚                  â”‚  â”‚                  â”‚
â”‚ â””â”€ metadata      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Stories EPIC-22 (21 points)

### ğŸŸ¢ Phase 1: MVP (5 pts) - Must-Have

#### Story 22.1: Table Metrics + Infrastructure (2 pts)

**Objectif**: Stocker mÃ©triques dans PostgreSQL

**Tasks**:
- [x] Migration: table `metrics` (id, timestamp, metric_type, metric_name, value, metadata)
- [x] Service: `MetricsCollector` (record_metric, collect_api_metrics, collect_redis_metrics)
- [x] Middleware: `MetricsMiddleware` (record latency per endpoint)
- [x] Endpoint: `/api/monitoring/metrics/record` (POST)

**Acceptance Criteria**:
- [x] Table `metrics` crÃ©Ã©e avec indexes (type, timestamp DESC)
- [x] MÃ©triques enregistrÃ©es automatiquement par request
- [x] Endpoint POST pour recording manuel

---

#### Story 22.2: Dashboard UnifiÃ© UI (2 pts)

**Objectif**: Page `/ui/monitoring/advanced` (one-pager)

**Features**:
- 4 KPI cards (API latency, Redis hit rate, PostgreSQL connections, System memory)
- Timeline latency (derniÃ¨re 1h)
- Top 5 slow queries
- Redis memory gauge
- Auto-refresh HTMX (5s)

**Acceptance Criteria**:
- [x] Page `/ui/monitoring/advanced` accessible
- [x] HTMX polling `/api/monitoring/advanced/summary` (5s)
- [x] ECharts timeline latency P50/P95
- [x] SCADA theme cohÃ©rent

---

#### Story 22.3: Logs Streaming Temps RÃ©el (1 pt)

**Objectif**: Stream logs via SSE

**Features**:
- Endpoint `/api/monitoring/logs/stream` (SSE)
- UI: Live logs avec auto-scroll
- Filtres: level (ERROR, WARN, INFO, DEBUG), service, search text
- Colorisation par level

**Acceptance Criteria**:
- [x] SSE stream fonctionne
- [x] Logs affichÃ©s en temps rÃ©el
- [x] Filtres fonctionnent cÃ´tÃ© client (JavaScript)
- [x] Auto-scroll activable

---

### ğŸŸ¡ Phase 2: Standard (8 pts) - Should-Have

#### Story 22.4: Redis Monitoring DÃ©taillÃ© (2 pts)

**Objectif**: Breakdown Redis par cache type

**Features**:
- Endpoint `/api/monitoring/redis/detailed`
- Hit rate par type (search_results, embeddings, graph_traversal)
- Memory usage (used/max/percent)
- Keys count par type (prefix analysis)
- Evictions (last 1h)
- Top keys (SCAN avec limit)

**Acceptance Criteria**:
- [x] Endpoint retourne JSON avec breakdown
- [x] UI: Card "Redis" avec 3 sub-cards (search, embeddings, graph)
- [x] Gauge memory avec alerte si > 80%

---

#### Story 22.5: API Performance Par Endpoint (3 pts)

**Objectif**: Latency breakdown par endpoint

**Features**:
- Endpoint `/api/monitoring/api/endpoints`
- Latency P50/P95/P99 par endpoint (last 1h)
- Throughput (req/s) par endpoint
- Error rate par endpoint
- Top 5 erreurs avec count
- Heatmap latency (endpoint Ã— heure)

**Acceptance Criteria**:
- [x] MÃ©triques agrÃ©gÃ©es depuis table `metrics`
- [x] UI: Table triable par latency/errors
- [x] Heatmap ECharts (endpoint Ã— heure)
- [x] P95 latency par endpoint visible

---

#### Story 22.6: Request Tracing (2 pts)

**Objectif**: Trace_id end-to-end

**Features**:
- Middleware `TraceIDMiddleware` (inject trace_id UUID)
- Header `X-Trace-ID` dans response
- Logs structlog avec trace_id
- UI: Logs stream affiche trace_id cliquable
- Click trace_id â†’ filter logs par trace_id

**Acceptance Criteria**:
- [x] Tous les logs ont trace_id
- [x] Header X-Trace-ID prÃ©sent
- [x] UI logs: clic sur trace_id filtre automatiquement

---

#### Story 22.7: Smart Alerting (1 pt)

**Objectif**: Alertes automatiques

**Features**:
- Service `AlertingService` (check_thresholds)
- Alerts table (id, alert_type, severity, message, timestamp, acknowledged)
- Endpoint `/api/monitoring/alerts` (GET active alerts)
- UI: Badge rouge dans navbar (count alerts)
- Clic badge â†’ modal avec liste alerts

**Seuils**:
- Cache hit rate < 70% â†’ Warning
- Memory > 80% â†’ Critical
- Slow queries > 1s â†’ Warning
- Evictions > 100/h â†’ Info
- Error rate > 5% â†’ Critical

**Acceptance Criteria**:
- [x] Alertes crÃ©Ã©es automatiquement (background task 1min)
- [x] UI badge affiche count
- [x] Alertes archivables (acknowledged=true)

---

### ğŸ”µ Phase 3: Nice-to-Have (8 pts)

#### Story 22.8: Historical Trends (2 pts)

**Objectif**: Comparaison temporelle

**Features**:
- Endpoint `/api/monitoring/trends`
- Comparison: now vs 1h ago vs 1d ago vs 1w ago
- Metrics: latency, hit rate, connections, error rate
- UI: Delta % avec flÃ¨ches (â†‘ rouge, â†“ vert)

**Acceptance Criteria**:
- [x] Trends calculÃ©s depuis table `metrics`
- [x] UI: Cards affichent delta %
- [x] Warning si trend > +20%

---

#### Story 22.9: Slow Query Explain Auto (2 pts)

**Objectif**: EXPLAIN automatique pour slow queries

**Features**:
- DÃ©tection slow query (> 1s)
- EXPLAIN ANALYZE automatique
- Stockage plan dans metadata
- UI: Affichage plan + suggestions d'index
- Suggestions: "Missing index on column X"

**Acceptance Criteria**:
- [x] Slow queries capturÃ©es avec EXPLAIN
- [x] UI: Bouton "View Plan" pour chaque slow query
- [x] Suggestions index affichÃ©es

---

#### Story 22.10: Export & Reporting (1 pt)

**Objectif**: Export metrics

**Features**:
- Endpoint `/api/monitoring/export?format=csv&period=24h`
- Formats: CSV, JSON
- UI: Bouton "Export" dans dashboard

**Acceptance Criteria**:
- [x] CSV/JSON tÃ©lÃ©chargeable
- [x] Periods: 1h, 24h, 7d, 30d

---

#### Story 22.11: Prometheus Export (2 pts)

**Objectif**: MÃ©triques format Prometheus (optionnel)

**Features**:
- Endpoint `/metrics` (Prometheus format)
- Exposition: counters, gauges, histograms
- Compatible Grafana

**Acceptance Criteria**:
- [x] Endpoint `/metrics` retourne Prometheus format
- [x] Importable dans Grafana

---

#### Story 22.12: Anomaly Detection (1 pt)

**Objectif**: DÃ©tection anomalies (ML simple)

**Features**:
- Algorithme: Moving Average + 3Ïƒ (Ã©cart-type)
- MÃ©triques surveillÃ©es: latency, hit rate, error rate
- UI: Badge "Anomaly Detected" si deviation > 3Ïƒ

**Acceptance Criteria**:
- [x] Anomalies dÃ©tectÃ©es automatiquement
- [x] UI: Badge orange si anomaly
- [x] Details: "Latency P95: 450ms (expected: 120ms, +275%)"

---

## ğŸ¨ UI/UX Mockup: `/ui/monitoring/advanced`

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ›ï¸  MnemoLite Advanced Monitoring                      [Auto-Refresh: 5s âœ“]  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                               â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚ â”‚ ğŸ“Š API       â”‚ â”‚ ğŸ”´ Redis     â”‚ â”‚ ğŸ˜ PostgreSQLâ”‚ â”‚ ğŸ’» System    â”‚        â”‚
â”‚ â”‚ P95: 85ms    â”‚ â”‚ Hit: 87.3%   â”‚ â”‚ Conn: 8/20   â”‚ â”‚ RAM: 45%     â”‚        â”‚
â”‚ â”‚ 15 req/s     â”‚ â”‚ 245 MB / 2GB â”‚ â”‚ Cache: 98.5% â”‚ â”‚ CPU: 28%     â”‚        â”‚
â”‚ â”‚ â†“ -5ms (vs1h)â”‚ â”‚ â†‘ Evict: 23  â”‚ â”‚ Slow: 3/h    â”‚ â”‚ Disk: 62%    â”‚        â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                                                               â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ ğŸ“ˆ API Latency P95 by Endpoint (Last 1h)                  [Heatmap View]â”‚ â”‚
â”‚ â”‚                                                                         â”‚ â”‚
â”‚ â”‚    [Line chart: P50, P95, P99 pour top 5 endpoints]                    â”‚ â”‚
â”‚ â”‚    /v1/code/search/hybrid: P95 = 85ms                                  â”‚ â”‚
â”‚ â”‚    /v1/code/search/lexical: P95 = 42ms                                 â”‚ â”‚
â”‚ â”‚    /v1/code/graph/traverse: P95 = 120ms                                â”‚ â”‚
â”‚ â”‚                                                                         â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                               â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ ğŸ”´ Redis Breakdown            â”‚ â”‚ ğŸ˜ Top 5 Slow Queries                 â”‚ â”‚
â”‚ â”‚                               â”‚ â”‚                                       â”‚ â”‚
â”‚ â”‚ search_results:               â”‚ â”‚ 1. SELECT * FROM code_chunks          â”‚ â”‚
â”‚ â”‚   Hit: 92%  Keys: 342         â”‚ â”‚    WHERE name LIKE ...                â”‚ â”‚
â”‚ â”‚                               â”‚ â”‚    â± 245ms (5 calls)                  â”‚ â”‚
â”‚ â”‚ embeddings:                   â”‚ â”‚    [View EXPLAIN â†’]                   â”‚ â”‚
â”‚ â”‚   Hit: 78%  Keys: 105         â”‚ â”‚                                       â”‚ â”‚
â”‚ â”‚                               â”‚ â”‚ 2. INSERT INTO events ...             â”‚ â”‚
â”‚ â”‚ graph_traversal:              â”‚ â”‚    â± 180ms (12 calls)                 â”‚ â”‚
â”‚ â”‚   Hit: 95%  Keys: 89          â”‚ â”‚                                       â”‚ â”‚
â”‚ â”‚                               â”‚ â”‚ [...3 more queries]                   â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                               â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ ğŸ“ Live Logs (SSE Stream)                                    [Pause] [â–¼] â”‚ â”‚
â”‚ â”‚ [Level: ALL â–¾] [Service: ALL â–¾] [Search: ____________] [Export CSV]    â”‚ â”‚
â”‚ â”‚                                                                         â”‚ â”‚
â”‚ â”‚ 21:50:12 [INFO ] trace=abc123 search_completed query="validate" 45ms   â”‚ â”‚
â”‚ â”‚ 21:50:11 [DEBUG] trace=abc123 cache_hit key="search:validate..." 28s   â”‚ â”‚
â”‚ â”‚ 21:50:10 [WARN ] trace=def456 slow_query duration=120ms table="code_ch"â”‚ â”‚
â”‚ â”‚ 21:50:05 [ERROR] trace=def456 timeout endpoint="/v1/code/search" 5002msâ”‚ â”‚
â”‚ â”‚                                                                         â”‚ â”‚
â”‚ â”‚ [Auto-scroll âœ“] [Show trace_id âœ“] [Color by level âœ“]                  â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                               â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ ğŸš¨ Active Alerts (2)                                                    â”‚ â”‚
â”‚ â”‚                                                                         â”‚ â”‚
â”‚ â”‚ âš ï¸  WARNING - Redis evictions high: 150/h (threshold: 100/h)            â”‚ â”‚
â”‚ â”‚     Suggestion: Consider increasing Redis maxmemory or TTL              â”‚ â”‚
â”‚ â”‚     [Acknowledge]                                                       â”‚ â”‚
â”‚ â”‚                                                                         â”‚ â”‚
â”‚ â”‚ âš ï¸  WARNING - Slow queries detected: 5 queries > 1s (last 1h)           â”‚ â”‚
â”‚ â”‚     Suggestion: View slow queries table above for details               â”‚ â”‚
â”‚ â”‚     [Acknowledge]                                                       â”‚ â”‚
â”‚ â”‚                                                                         â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ’¡ IdÃ©es SupplÃ©mentaires (Brainstorm)

### 1. **Health Check DÃ©taillÃ© (Health Dashboard)**

**Feature**: Page `/ui/health` (public, no auth)

**Contenu**:
- âœ…/âŒ PostgreSQL (latency < 10ms)
- âœ…/âŒ Redis (latency < 5ms)
- âœ…/âŒ Disk space (> 10% free)
- âœ…/âŒ Memory (< 80% used)
- Overall status: ğŸŸ¢ Operational | ğŸŸ¡ Degraded | ğŸ”´ Critical

**Use Case**: Status page pour monitoring externe (UptimeRobot, etc.)

---

### 2. **Query Plan Visualizer**

**Feature**: Visualisation plan d'exÃ©cution PostgreSQL

**UI**:
- Slow query dÃ©tectÃ©e â†’ Clic "View Plan"
- Modal affiche plan d'exÃ©cution formatÃ©
- Highlight: Seq Scan (rouge), Index Scan (vert)
- Suggestions: "Add index on column X"

**Example**:
```
Query: SELECT * FROM code_chunks WHERE name LIKE '%validate%'

Plan:
â””â”€ Seq Scan on code_chunks (cost=0..1000 rows=100)  ğŸ”´ SLOW
   Filter: (name ~~ '%validate%')

Suggestion:
CREATE INDEX idx_code_chunks_name_trgm
ON code_chunks USING gin (name gin_trgm_ops);
```

---

### 3. **WebSocket Events (Alternative SSE)**

**Feature**: Stream logs via WebSocket au lieu de SSE

**Avantages**:
- Bidirectionnel (client peut envoyer commandes)
- Moins de overhead que SSE
- Compatible tous browsers

**Use Case**: Si SSE pose problÃ¨me (proxies, timeouts)

---

### 4. **Cache Warmup Manual**

**Feature**: Endpoint `/v1/cache/warmup`

**Params**:
- `repository`: Warm up cache for specific repository
- `popular_queries`: Warm up most popular queries (top 10)

**Use Case**: AprÃ¨s cache flush, reconstruire cache rapidement

---

### 5. **Query Performance Index**

**Feature**: Score global performance

**Calcul**:
```
Performance Index = (
    0.4 Ã— Cache Hit Rate +
    0.3 Ã— (100 - Avg Latency P95 / 10) +
    0.2 Ã— (100 - Error Rate) +
    0.1 Ã— (100 - Memory %)
) / 100

Example:
- Cache Hit: 87% â†’ 0.4 Ã— 87 = 34.8
- Latency P95: 85ms â†’ 0.3 Ã— (100 - 8.5) = 27.45
- Error Rate: 0.5% â†’ 0.2 Ã— 99.5 = 19.9
- Memory: 45% â†’ 0.1 Ã— 55 = 5.5

Performance Index = 87.65/100 â†’ Grade: B+
```

**UI**: Jauge + grade (A+, A, B, C, D, F)

---

### 6. **Distributed Tracing (OpenTelemetry - Nice-to-Have)**

**Feature**: Traces distribuÃ©es (si multi-service plus tard)

**Stack**: OpenTelemetry + Tempo

**Use Case**: Uniquement si architecture devient multi-services

**Verdict**: â¸ï¸ **YAGNI** pour l'instant

---

### 7. **Performance Regression Detection**

**Feature**: DÃ©tection rÃ©gression entre versions

**Workflow**:
1. Tag metrics avec version (git commit hash)
2. Compare P95 latency avant/aprÃ¨s deploy
3. Alert si regression > 20%

**Example**:
```
ğŸš¨ Performance Regression Detected!
Version: abc123 â†’ def456
Endpoint: /v1/code/search/hybrid
Latency P95: 85ms â†’ 125ms (+47%)  ğŸ”´
Recommendation: Rollback or investigate
```

---

### 8. **Capacity Planning**

**Feature**: Projection future usage

**Metrics**:
- DB size growth (MB/day)
- Redis memory growth
- Request rate trend

**UI**:
```
Capacity Projections (Next 30 Days):
â”œâ”€ DB Size: 150 MB â†’ 180 MB (+20%)
â”œâ”€ Redis Memory: 245 MB â†’ 290 MB (+18%)
â””â”€ Req Rate: 15 req/s â†’ 20 req/s (+33%)

Warnings:
âš ï¸  Redis will reach 90% capacity in 45 days
    Recommendation: Increase maxmemory to 3GB
```

---

### 9. **Session Recording (Request Replay)**

**Feature**: Enregistrer requÃªtes pour replay

**Workflow**:
1. Capture request (method, path, body, headers)
2. Store in table `request_history`
3. UI: "Replay" button â†’ re-execute request
4. Compare response time before/after

**Use Case**: Testing perf aprÃ¨s optimisation

---

### 10. **Custom Dashboards**

**Feature**: Builder de dashboard custom

**UI**:
- Drag & drop widgets (chart, gauge, table)
- Save layouts per user
- Export/import dashboard config (JSON)

**Use Case**: Chaque user peut crÃ©er son propre dashboard

---

## âš ï¸ ConsidÃ©rations Techniques

### 1. Performance Impact

**Recording MÃ©triques**:
- Overhead: ~1-2ms par request
- Insertion DB: async (non-bloquant)
- Batch inserts: toutes les 10s (rÃ©duire load DB)

**Optimisations**:
- Table `metrics` partitionnÃ©e par jour (si volume Ã©levÃ©)
- Indexes: (metric_type, timestamp DESC)
- Retention: 30 jours (auto-cleanup)

---

### 2. SÃ©curitÃ©

**AccÃ¨s Dashboard**:
- `/ui/monitoring/advanced` = **Admin only**
- Authentication requise (RBAC)
- API endpoints protÃ©gÃ©s

**Logs**:
- Ne jamais logger passwords/tokens
- Anonymiser PII (emails, IPs)
- Filtrer sensitive data

---

### 3. ScalabilitÃ©

**Single Instance** (actuel):
- âœ… Table `metrics` PostgreSQL
- âœ… Agregation en mÃ©moire
- âœ… HTMX polling

**Multi-Instance** (futur):
- â¸ï¸ Besoin Prometheus + Grafana
- â¸ï¸ Shared Redis pour mÃ©triques
- â¸ï¸ Distributed tracing (Tempo)

**Verdict**: YAGNI pour l'instant, architecture actuelle suffisante

---

### 4. Retention Policy

**Metrics**:
- Keep: 30 jours
- Cleanup: Cron job daily (`DELETE FROM metrics WHERE timestamp < NOW() - INTERVAL '30 days'`)

**Logs**:
- Keep: 7 jours
- Rotation: logrotate ou docker logs driver

---

## ğŸ“Š Estimation Points & Timeline

| Phase | Stories | Points | Timeline |
|-------|---------|--------|----------|
| **Phase 1: MVP** | 22.1-22.3 | 5 pts | 1 semaine |
| **Phase 2: Standard** | 22.4-22.7 | 8 pts | 2 semaines |
| **Phase 3: Nice-to-Have** | 22.8-22.12 | 8 pts | 2 semaines |
| **Total** | 12 stories | **21 pts** | **5 semaines** |

---

## ğŸ¯ Recommandation

### Phase 1: MVP (5 pts) - âœ… **Ã€ FAIRE MAINTENANT**

**Objectif**: Monitoring basique production-ready

**Features**:
1. Table `metrics` + infrastructure
2. Dashboard `/ui/monitoring/advanced` unifiÃ©
3. Logs streaming temps rÃ©el (SSE)

**Valeur**:
- âœ… VisibilitÃ© immÃ©diate production
- âœ… DÃ©tection rapide problÃ¨mes
- âœ… Zero dÃ©pendance externe
- âœ… KISS

**Timeline**: 1 semaine (5 jours)

---

### Phase 2: Standard (8 pts) - âœ… **RECOMMANDÃ‰**

**Objectif**: Monitoring production-grade

**Features**:
1. Redis monitoring dÃ©taillÃ© (breakdown par type)
2. API performance par endpoint (P50/P95/P99)
3. Request tracing (trace_id)
4. Smart alerting (seuils + notifications)

**Valeur**:
- âœ… Diagnostic approfondi
- âœ… Optimisations guidÃ©es par data
- âœ… Alertes proactives
- âœ… TraÃ§abilitÃ© end-to-end

**Timeline**: 2 semaines

---

### Phase 3: Nice-to-Have (8 pts) - â¸ï¸ **YAGNI (pour l'instant)**

**Uniquement si**:
- Multi-instance deployment
- Ã‰quipe DevOps dÃ©diÃ©e
- Besoin traces distribuÃ©es
- Budget temps disponible

**Sinon**: â¸ï¸ Reporter aprÃ¨s Phase 2

---

## ğŸ”— Comparaison avec Alternatives

### MnemoLite EPIC-22 (Approche KISS)

**Pros**:
- âœ… Zero dÃ©pendance externe
- âœ… IntÃ©grÃ© dans UI native
- âœ… HTMX + ECharts (dÃ©jÃ  prÃ©sents)
- âœ… 1 dashboard = tout visible
- âœ… Maintenance minimale

**Cons**:
- âŒ Pas de visualisations Grafana
- âŒ Pas de clustering multi-instance (pour l'instant)

---

### Prometheus + Grafana

**Pros**:
- âœ… Industry standard
- âœ… Dashboards prÃ©-faits
- âœ… Alerting puissant
- âœ… Multi-instance ready

**Cons**:
- âŒ 2+ services supplÃ©mentaires (Prometheus, Grafana)
- âŒ Maintenance overhead
- âŒ Pas intÃ©grÃ© UI MnemoLite
- âŒ ComplexitÃ© accrue

**Verdict**: â¸ï¸ **YAGNI** (sauf si multi-instance)

---

### ELK Stack (Elasticsearch + Logstash + Kibana)

**Pros**:
- âœ… Logs centralisÃ©s
- âœ… Full-text search puissant
- âœ… Visualisations Kibana

**Cons**:
- âŒ 3 services (Elastic, Logstash, Kibana)
- âŒ Lourd (>2GB RAM)
- âŒ ComplexitÃ© extrÃªme

**Verdict**: â¸ï¸ **OVERKILL** pour MnemoLite

---

## ğŸ‰ Conclusion

**EPIC-22 = Production-Ready Observability Sans ComplexitÃ©**

**Principe**:
- **EXTEND > REBUILD** (leverage existant)
- **KISS** (zero Prometheus/Grafana/ELK)
- **YAGNI** (Phase 1 = 80% valeur, 20% effort)

**Next Step**: CrÃ©er `EPIC-22_README.md` avec plan d'action dÃ©taillÃ©

---

**CrÃ©Ã©**: 2025-10-24
**Auteur**: Claude Code + Brainstorming User
**Status**: ğŸ§  Awaiting approval
**Version**: 1.0.0
