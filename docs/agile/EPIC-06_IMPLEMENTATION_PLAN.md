# EPIC-06: Plan d'Impl√©mentation D√©taill√© - Code Intelligence

**Version**: 1.0.0
**Date**: 2025-10-15
**Statut**: üìã PLANIFICATION APPROFONDIE
**Auteur**: Deep Analysis + Web Research 2024-2025

---

## üéØ Executive Summary

**Objectif**: Ajouter capacit√©s code intelligence √† MnemoLite TOUT EN pr√©servant use case principal (agent memory).

**Strat√©gie**: Architecture dual-purpose avec tables s√©par√©es (`events` + `code_chunks`) et dual embeddings (nomic-text + jina-code).

**Dur√©e totale**: **13 semaines** (1 semaine Phase 0 + 12 semaines Phases 1-4)

**Story points total**: **74 points** (Phase 0: 8pts, Phase 1-4: 66pts)

**Complexit√©**: ‚≠ê‚≠ê‚≠ê‚≠ê‚ö™ (4/5 - HAUTE)

---

## ‚ö†Ô∏è Contraintes Critiques Valid√©es

### Contraintes Techniques
- ‚úÖ **PostgreSQL 18 only**: Pas d'autres DB, pas de services externes
- ‚úÖ **100% local**: Aucune d√©pendance cloud/API (embeddings locaux)
- ‚úÖ **CPU-friendly**: D√©ploiement sans GPU requis
- ‚úÖ **RAM budget**: Total embeddings < 1 GB (target: ~700 MB)
- ‚úÖ **768D embeddings**: Compatibilit√© infrastructure existante (pas de migration DB!)
- ‚úÖ **Async-first**: SQLAlchemy 2.0 async, asyncpg
- ‚úÖ **Backward compatible**: Table `events` intacte, API v1 sans breaking changes

### Contraintes Fonctionnelles
- ‚úÖ **Use case principal**: Agent memory (conversations, docs) doit rester fonctionnel
- ‚úÖ **Use case nouveau**: Code intelligence (indexation, search, graph)
- ‚úÖ **Architecture**: Tables s√©par√©es pour s√©paration claire des concerns

---

## üî¨ Recherches & Validations Techniques

### 1. Tree-sitter Integration (Valid√© 2024)

**Package recommand√©**: `tree-sitter-languages` (pre-compiled wheels)

**Avantages**:
- Pas de compilation manuelle requise
- Wheels pr√©-compil√©s pour toutes plateformes
- Support multi-langages out-of-the-box
- Utilis√© par CodeTF (Salesforce) - production-ready

**Pattern d'int√©gration**:
```python
from tree_sitter_languages import get_parser, get_language

parser = get_parser('python')
tree = parser.parse(bytes(source_code, 'utf8'))
```

**Langages prioritaires**: Python, JavaScript, TypeScript, Go, Rust, Java

**R√©f√©rence**: https://pypi.org/project/tree-sitter-languages/

---

### 2. Dual Embeddings Memory Management (Valid√© 2024)

**Strat√©gie**: Chargement simultan√© de 2 mod√®les sentence-transformers

**Mod√®les**:
1. `nomic-ai/nomic-embed-text-v1.5` (137M params, ~260 MB RAM)
2. `jinaai/jina-embeddings-v2-base-code` (161M params, ~400 MB RAM)

**Total RAM**: ~700 MB (sous budget 1 GB)

**Optimisations possibles**:
- FP16 precision: -50% RAM (non requis pour notre budget)
- Batch processing: 32-64 samples/batch (optimal CPU)
- Lazy loading: Charger mod√®les seulement quand n√©cessaire

**Pattern d'impl√©mentation**:
```python
class EmbeddingService:
    def __init__(self):
        self.text_model = SentenceTransformer("nomic-ai/nomic-embed-text-v1.5")
        self.code_model = SentenceTransformer("jinaai/jina-embeddings-v2-base-code")

    async def generate_embedding(self, text: str, domain: EmbeddingDomain):
        if domain == EmbeddingDomain.TEXT:
            return self.text_model.encode(text).tolist()
        elif domain == EmbeddingDomain.CODE:
            return self.code_model.encode(text).tolist()
        elif domain == EmbeddingDomain.HYBRID:
            return {
                'text': self.text_model.encode(text).tolist(),
                'code': self.code_model.encode(text).tolist()
            }
```

**R√©f√©rence**: https://milvus.io/ai-quick-reference/how-can-you-reduce-the-memory-footprint-of-sentence-transformer-models

---

### 3. BM25-like Search PostgreSQL (‚ö†Ô∏è CRITIQUE)

**D√©couverte importante**: PostgreSQL natif **NE SUPPORTE PAS BM25**!

**Raison**: Algorithmes BM25/TF-IDF n√©cessitent statistiques globales (fr√©quence termes corpus) non disponibles dans ranking functions PostgreSQL.

**Alternatives valid√©es**:

#### Option A: pg_trgm Similarity (RECOMMAND√â Phase 1-2)
```sql
-- Extension native PostgreSQL
CREATE EXTENSION pg_trgm;

-- Index trigram
CREATE INDEX idx_code_source_trgm ON code_chunks USING gin (source_code gin_trgm_ops);

-- Requ√™te similarity
SELECT id, name, source_code,
       similarity(source_code, 'query text') AS score
FROM code_chunks
WHERE source_code % 'query text'  -- % operator = similarity match
ORDER BY score DESC
LIMIT 10;
```

**Avantages**:
- ‚úÖ Extension native PostgreSQL (d√©j√† disponible)
- ‚úÖ Pas de d√©pendances externes
- ‚úÖ Excellent pour fuzzy matching (typos)
- ‚úÖ Performance correcte avec index GIN

**Limites**:
- ‚ö†Ô∏è Pas de TF-IDF (pas de poids par terme)
- ‚ö†Ô∏è Similarit√© simple (pas de ranking BM25)

#### Option B: PostgreSQL Full-Text Search (tsvector)
```sql
-- Index full-text
CREATE INDEX idx_code_source_fts ON code_chunks USING gin (to_tsvector('english', source_code));

-- Requ√™te FTS
SELECT id, name, source_code,
       ts_rank(to_tsvector('english', source_code), to_tsquery('function & calculate')) AS rank
FROM code_chunks
WHERE to_tsvector('english', source_code) @@ to_tsquery('function & calculate')
ORDER BY rank DESC;
```

**Avantages**:
- ‚úÖ Ranking natif (ts_rank, ts_rank_cd)
- ‚úÖ Support op√©rateurs bool√©ens (AND, OR, NOT)

**Limites**:
- ‚ö†Ô∏è Optimis√© pour texte naturel (pas code)
- ‚ö†Ô∏è Stemming probl√©matique pour code

#### Option C: Extensions BM25 Externes (√âvaluation Phase 3)

1. **pg_search (ParadeDB)**: Extension Rust, vrai BM25 complet
   - R√©f√©rence: https://blog.paradedb.com/pages/introducing_search
   - ‚ö†Ô∏è D√©pendance externe (compilation Rust)
   - ‚ö†Ô∏è Int√©gration compl√®te (BM25 + hybrid int√©gr√©)

2. **VectorChord-BM25 (TensorChord)**: Rust extension, vrai BM25 avec Block-WeakAnd
   - R√©f√©rence: https://github.com/tensorchord/VectorChord
   - ‚ö†Ô∏è D√©pendance externe (compilation Rust)
   - ‚úÖ Performance excellente (Block-WeakAnd = early termination)
   - ‚ö†Ô∏è Moins mature que pgvector

3. **plpgsql_bm25**: Impl√©mentation BM25 pure PL/pgSQL
   - R√©f√©rence: https://github.com/paradedb/paradedb/discussions/1584
   - ‚úÖ **Aucune compilation** (pure SQL)
   - ‚úÖ **Aucune d√©pendance externe** (PostgreSQL natif uniquement)
   - ‚ö†Ô∏è Performance moyenne (PL/pgSQL plus lent que Rust)
   - ‚ö†Ô∏è Calcul TF/IDF manuel requis (tables statistiques)

**D√âCISION ARCHITECTURE**:
- **Phase 1-2**: Utiliser **pg_trgm similarity** (simple, natif, performant)
- **Phase 3 (v1.4.0)**: Hybrid search = pg_trgm (lexical) + pgvector (semantic) + RRF fusion
- **Post-Phase 3 Benchmark**: √âvaluer alternatives BM25 si **Recall@10 < 80%**
  - **Option pr√©f√©r√©e**: plpgsql_bm25 (pur SQL, respect contrainte "PostgreSQL natif")
  - **Option performance**: VectorChord-BM25 (si d√©pendances externes accept√©es)
  - **Option compl√®te**: pg_search/ParadeDB (si BM25 + hybrid int√©gr√© souhait√©)

**R√©f√©rences**:
- Hybrid Search PostgreSQL: https://jkatz05.com/post/postgres/hybrid-search-postgres-pgvector/
- VectorChord-BM25: https://github.com/tensorchord/VectorChord
- plpgsql_bm25: https://github.com/paradedb/paradedb/discussions/1584
- pg_search: https://blog.paradedb.com/pages/introducing_search

---

### 4. Alembic Migrations Async (Valid√© 2024)

**Setup initial**:
```bash
# Initialiser avec template async
alembic init -t async migrations

# Configuration env.py
from sqlalchemy.ext.asyncio import create_async_engine

async def run_async_migrations():
    connectable = create_async_engine(config.get_main_option("sqlalchemy.url"))

    async with connectable.connect() as connection:
        await connection.run_sync(do_run_migrations)
```

**Pattern ajout nouvelle table**:
```python
# alembic revision --autogenerate -m "Add code_chunks table"

def upgrade() -> None:
    op.create_table(
        'code_chunks',
        sa.Column('id', sa.UUID(), nullable=False),
        sa.Column('file_path', sa.Text(), nullable=False),
        sa.Column('language', sa.Text(), nullable=False),
        sa.Column('source_code', sa.Text(), nullable=False),
        sa.Column('embedding_text', pgvector.sqlalchemy.Vector(768)),
        sa.Column('embedding_code', pgvector.sqlalchemy.Vector(768)),
        sa.Column('metadata', postgresql.JSONB()),
        sa.PrimaryKeyConstraint('id')
    )

    # Index HNSW
    op.execute("""
        CREATE INDEX idx_code_embedding_text ON code_chunks
        USING hnsw (embedding_text vector_cosine_ops)
        WITH (m = 16, ef_construction = 64);
    """)

    op.execute("""
        CREATE INDEX idx_code_embedding_code ON code_chunks
        USING hnsw (embedding_code vector_cosine_ops)
        WITH (m = 16, ef_construction = 64);
    """)

def downgrade() -> None:
    op.drop_table('code_chunks')
```

**‚ö†Ô∏è ATTENTION**: Toujours **review manually** les migrations autogener√©es!

**R√©f√©rence**: https://testdriven.io/blog/fastapi-sqlmodel/

---

## üìä Architecture Finale Valid√©e

### Sch√©ma DB

```sql
-- Table 1: events (INCHANG√âE - agent memory)
CREATE TABLE events (
    id UUID PRIMARY KEY,
    timestamp TIMESTAMPTZ NOT NULL,
    content JSONB NOT NULL,
    embedding VECTOR(768),  -- nomic-embed-text-v1.5 uniquement
    metadata JSONB
);

-- Index existants (pas de changement)
CREATE INDEX idx_events_embedding ON events
USING hnsw (embedding vector_cosine_ops) WITH (m = 16, ef_construction = 64);

CREATE INDEX idx_events_metadata_gin ON events USING gin (metadata jsonb_path_ops);
CREATE INDEX idx_events_timestamp ON events (timestamp);

---

-- Table 2: code_chunks (NOUVELLE - code intelligence)
CREATE TABLE code_chunks (
    -- Identification
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    file_path TEXT NOT NULL,
    language TEXT NOT NULL,  -- 'python', 'javascript', 'typescript', etc.
    chunk_type TEXT NOT NULL,  -- 'function', 'class', 'method', 'module'
    name TEXT,  -- Nom fonction/classe

    -- Contenu
    source_code TEXT NOT NULL,
    start_line INT,
    end_line INT,

    -- Dual embeddings (768D chacun!)
    embedding_text VECTOR(768),  -- nomic-text (docstrings, comments)
    embedding_code VECTOR(768),  -- jina-code (code s√©mantique)

    -- M√©tadonn√©es code
    metadata JSONB NOT NULL,  -- {complexity, params, returns, docstring, tests, etc.}

    -- Timestamps
    indexed_at TIMESTAMPTZ DEFAULT NOW(),
    last_modified TIMESTAMPTZ,

    -- Relations
    node_id UUID,  -- Lien vers nodes table (graph)
    repository TEXT,
    commit_hash TEXT
);

-- Index HNSW pour dual embeddings
CREATE INDEX idx_code_embedding_text ON code_chunks
USING hnsw (embedding_text vector_cosine_ops) WITH (m = 16, ef_construction = 64);

CREATE INDEX idx_code_embedding_code ON code_chunks
USING hnsw (embedding_code vector_cosine_ops) WITH (m = 16, ef_construction = 64);

-- Index GIN m√©tadonn√©es
CREATE INDEX idx_code_metadata ON code_chunks USING gin (metadata jsonb_path_ops);

-- Index B-tree filtrage
CREATE INDEX idx_code_language ON code_chunks (language);
CREATE INDEX idx_code_type ON code_chunks (chunk_type);
CREATE INDEX idx_code_file ON code_chunks (file_path);
CREATE INDEX idx_code_indexed_at ON code_chunks (indexed_at);

-- Index trigram pour pg_trgm similarity search
CREATE EXTENSION IF NOT EXISTS pg_trgm;
CREATE INDEX idx_code_source_trgm ON code_chunks USING gin (source_code gin_trgm_ops);
CREATE INDEX idx_code_name_trgm ON code_chunks USING gin (name gin_trgm_ops);
```

### Services Layer

```python
# api/services/embedding_service.py
from enum import Enum
from sentence_transformers import SentenceTransformer

class EmbeddingDomain(str, Enum):
    TEXT = "text"      # Conversations, docs, general text
    CODE = "code"      # Code snippets, functions, classes
    HYBRID = "hybrid"  # Generate both (for code with docstrings)

class EmbeddingService:
    def __init__(self, settings):
        # Model 1: General text (existing, ~260 MB)
        self.text_model = SentenceTransformer(settings.EMBEDDING_MODEL)
        # "nomic-ai/nomic-embed-text-v1.5"

        # Model 2: Code specialized (new, ~400 MB)
        self.code_model = SentenceTransformer(settings.CODE_EMBEDDING_MODEL)
        # "jinaai/jina-embeddings-v2-base-code"

        logger.info(f"Dual embeddings loaded: ~{self._estimate_ram()}MB total")

    def _estimate_ram(self) -> int:
        """Estimate total RAM usage (MB)."""
        return 260 + 400  # ~700 MB

    async def generate_embedding(
        self,
        text: str,
        domain: EmbeddingDomain = EmbeddingDomain.TEXT
    ) -> dict[str, list[float]]:
        """
        Generate embedding(s) based on domain.

        Returns:
            - TEXT domain: {'text': [...768D...]}
            - CODE domain: {'code': [...768D...]}
            - HYBRID domain: {'text': [...768D...], 'code': [...768D...]}
        """
        result = {}

        if domain in [EmbeddingDomain.TEXT, EmbeddingDomain.HYBRID]:
            emb = self.text_model.encode(text, convert_to_tensor=False)
            result['text'] = emb.tolist()

        if domain in [EmbeddingDomain.CODE, EmbeddingDomain.HYBRID]:
            emb = self.code_model.encode(text, convert_to_tensor=False)
            result['code'] = emb.tolist()

        return result
```

```python
# api/services/code_chunking_service.py
from tree_sitter_languages import get_parser, get_language

class CodeChunkingService:
    def __init__(self):
        self.parsers = {
            'python': get_parser('python'),
            'javascript': get_parser('javascript'),
            'typescript': get_parser('typescript'),
            'go': get_parser('go'),
            'rust': get_parser('rust'),
            'java': get_parser('java')
        }

    async def chunk_code(
        self,
        source_code: str,
        language: str,
        max_chunk_size: int = 2000
    ) -> list[CodeChunk]:
        """
        Semantic chunking via AST (cAST algorithm inspired).

        Algorithm:
        1. Parse code ‚Üí AST
        2. Identify code units (functions, classes, methods)
        3. Split large units recursively
        4. Merge small adjacent chunks
        5. Fallback to fixed chunking if parsing fails
        """
        try:
            parser = self.parsers.get(language)
            if not parser:
                logger.warning(f"No parser for {language}, falling back to fixed chunking")
                return self._fallback_chunking(source_code, max_chunk_size)

            tree = parser.parse(bytes(source_code, 'utf8'))
            code_units = self._identify_code_units(tree, language)

            chunks = []
            for unit in code_units:
                if unit.size <= max_chunk_size:
                    chunks.append(self._create_chunk(unit))
                else:
                    # Split recursively
                    sub_chunks = self._split_large_unit(unit, max_chunk_size)
                    chunks.extend(sub_chunks)

            # Merge small adjacent chunks
            merged = self._merge_small_chunks(chunks, min_size=100)
            return merged

        except Exception as e:
            logger.error(f"AST chunking failed: {e}, fallback to fixed")
            return self._fallback_chunking(source_code, max_chunk_size)
```

---

## üìã Plan d'Impl√©mentation D√©taill√©

### **PHASE 0: Infrastructure Setup (1 semaine)**

**Objectif**: Pr√©parer infrastructure sans casser l'existant

**Story Points**: 8

#### Story 0.1: Alembic Async Setup (3 points)

**Tasks**:
1. ‚úÖ Initialiser Alembic avec template async
   ```bash
   cd /home/giak/Work/MnemoLite
   alembic init -t async alembic
   ```

2. ‚úÖ Configurer `alembic.ini`
   - `sqlalchemy.url` = DATABASE_URL async (postgresql+asyncpg)
   - Script location
   - File template

3. ‚úÖ Configurer `alembic/env.py`
   - Import `AsyncEngine`
   - Pattern `run_sync` pour migrations
   - Import metadata `Base.metadata`

4. ‚úÖ Cr√©er migration initiale baseline
   ```bash
   alembic revision --autogenerate -m "Baseline: existing events table"
   ```

5. ‚úÖ Tester migration up/down
   ```bash
   alembic upgrade head
   alembic downgrade -1
   alembic upgrade head
   ```

**Acceptance Criteria**:
- ‚úÖ Alembic initialis√© avec template async
- ‚úÖ Migration baseline cr√©√©e et appliqu√©e
- ‚úÖ Tests migration up/down passent
- ‚úÖ Pas de r√©gression sur table `events` existante

**D√©pendances**:
- SQLAlchemy 2.0 async (d√©j√† install√©)
- asyncpg (d√©j√† install√©)
- alembic (√† ajouter requirements.txt)

---

#### Story 0.2: Dual Embeddings Service (5 points)

**Tasks**:
1. ‚úÖ Ajouter d√©pendances `requirements.txt`
   ```
   sentence-transformers>=2.7.0
   tree-sitter-languages>=1.10.0  # Pour Phase 1
   ```

2. ‚úÖ √âtendre `EmbeddingService` avec dual models
   - Cr√©er enum `EmbeddingDomain(TEXT, CODE, HYBRID)`
   - Ajouter `self.code_model`
   - M√©thode `generate_embedding(text, domain)`
   - M√©thode `_estimate_ram()`

3. ‚úÖ Ajouter variables environnement `.env.example`
   ```env
   # Embeddings (existing)
   EMBEDDING_MODEL=nomic-ai/nomic-embed-text-v1.5
   EMBEDDING_DIMENSION=768

   # Code embeddings (new)
   CODE_EMBEDDING_MODEL=jinaai/jina-embeddings-v2-base-code
   ```

4. ‚úÖ Tests unitaires dual embeddings
   - Test chargement 2 mod√®les
   - Test dimensions 768D identiques
   - Test domains TEXT/CODE/HYBRID
   - Test RAM usage < 1 GB

5. ‚úÖ Benchmark local
   - Latence g√©n√©ration (TEXT vs CODE vs HYBRID)
   - RAM usage r√©el (psutil)
   - Comparer similarity (text vs code embeddings sur code snippet)

**Acceptance Criteria**:
- ‚úÖ `EmbeddingService` supporte dual models
- ‚úÖ Enum `EmbeddingDomain` impl√©ment√©
- ‚úÖ Tests unitaires passent (coverage >85%)
- ‚úÖ Benchmark: RAM < 1 GB, latence < 100ms
- ‚úÖ Documentation `.env.example` mise √† jour

**D√©pendances**:
- sentence-transformers 2.7+
- Variables environnement CODE_EMBEDDING_MODEL

---

### **PHASE 1: Foundation Code (4 semaines)**

**Objectif**: Chunking + Dual Embeddings + Metadata + Table code_chunks

**Story Points**: 31 (13 + 5 + 8 + 5)

#### Story 1: Tree-sitter Integration & AST Chunking (13 points)

**Complexit√©**: HAUTE
**Dur√©e estim√©e**: 2 semaines

**Tasks**:
1. ‚úÖ Setup tree-sitter-languages
   - Installation `tree-sitter-languages`
   - V√©rification parsers disponibles (Python, JS, TS, Go, Rust, Java)

2. ‚úÖ Impl√©menter `CodeChunkingService`
   - D√©tection langage (extension fichier)
   - Parsing AST via tree-sitter
   - Identification code units (functions, classes, methods)
   - Algorithme split-then-merge (inspir√© cAST paper)
   - Fallback chunking fixe si parsing √©choue

3. ‚úÖ Cr√©er mod√®les Pydantic
   - `CodeChunk` (source_code, chunk_type, name, start_line, end_line, etc.)
   - `CodeUnit` (interm√©diaire AST)

4. ‚úÖ Tests unitaires
   - Test Python function chunking
   - Test JavaScript class chunking
   - Test TypeScript methods
   - Test large function split (>2000 chars)
   - Test small chunks merge (<100 chars)
   - Test fallback on invalid syntax
   - Test performance (<100ms pour 300 LOC)

5. ‚úÖ Documentation technique
   - Docstrings compl√®tes
   - README: comment ajouter nouveau langage
   - ADR: pourquoi tree-sitter vs alternatives

**Acceptance Criteria**:
- ‚úÖ Tree-sitter int√©gr√© (6+ langages)
- ‚úÖ >80% chunks = fonctions/classes compl√®tes
- ‚úÖ Performance <100ms pour fichier 300 LOC
- ‚úÖ Tests coverage >85%
- ‚úÖ Fallback robuste sur erreurs parsing

**D√©pendances**:
- tree-sitter-languages 1.10+

**R√©f√©rences**:
- cAST paper (arxiv 2024): 82% am√©lioration pr√©cision
- CodeTF: utilise tree-sitter comme core parser

---

#### Story 2: Dual Embeddings Integration (5 points - R√âDUIT)

**Complexit√©**: MOYENNE
**Dur√©e estim√©e**: 1 semaine

**Tasks**:
1. ‚úÖ Validation Setup Phase 0
   - V√©rifier dual models charg√©s
   - V√©rifier RAM < 1 GB

2. ‚úÖ Benchmark production
   - CodeSearchNet subset (100 queries)
   - Comparer nomic-text vs jina-code sur code
   - Mesurer Recall@10, latence, RAM

3. ‚úÖ Documentation
   - Guide utilisation domains (TEXT/CODE/HYBRID)
   - Benchmark report (r√©sultats chiffr√©s)
   - ADR: pourquoi jina-code vs nomic-code

**Acceptance Criteria**:
- ‚úÖ Benchmark nomic-text vs jina-code document√©
- ‚úÖ RAM total < 1 GB valid√©
- ‚úÖ Dimensions 768D identiques valid√©es
- ‚úÖ Documentation compl√®te

**D√©pendances**:
- Phase 0 Story 0.2 compl√©t√©e

---

#### Story 2bis: Code Chunks Table & Repository (5 points)

**Complexit√©**: MOYENNE
**Dur√©e estim√©e**: 1 semaine

**Tasks**:
1. ‚úÖ Migration Alembic `code_chunks` table
   ```bash
   alembic revision --autogenerate -m "Add code_chunks table"
   ```
   - Review manual migration
   - Ajouter index HNSW (embedding_text, embedding_code)
   - Ajouter index GIN (metadata)
   - Ajouter index B-tree (language, chunk_type, file_path)
   - Ajouter index trigram (source_code, name)

2. ‚úÖ Impl√©menter `CodeChunkRepository`
   - CRUD operations (create, get_by_id, update, delete)
   - `filter_by_metadata()`
   - `search_vector()` avec support dual embeddings
   - `search_similarity()` avec pg_trgm

3. ‚úÖ Cr√©er mod√®les Pydantic
   - `CodeChunkModel` (DB representation)
   - `CodeChunkCreate` (input)
   - `CodeChunkUpdate` (patch)

4. ‚úÖ Tests int√©gration PostgreSQL
   - Test CRUD operations
   - Test vector search (embedding_text vs embedding_code)
   - Test metadata filtering
   - Test pg_trgm similarity

**Acceptance Criteria**:
- ‚úÖ Table `code_chunks` cr√©√©e via migration
- ‚úÖ Repository impl√©ment√© (coverage >85%)
- ‚úÖ Tests int√©gration PostgreSQL passent
- ‚úÖ Index HNSW op√©rationnels (v√©rifier EXPLAIN ANALYZE)

**D√©pendances**:
- Phase 0 Story 0.1 (Alembic setup)
- pgvector extension (d√©j√† install√©)
- pg_trgm extension

---

#### Story 3: Code Metadata Extraction (8 points)

**Complexit√©**: MOYENNE
**Dur√©e estim√©e**: 1-2 semaines

**Tasks**:
1. ‚úÖ Impl√©menter `CodeMetadataService`
   - Extraction signature (params, returns, type hints)
   - Extraction docstring (Google/NumPy/Sphinx styles)
   - Calcul complexit√© (cyclomatic via `radon`, LOC, cognitive)
   - Extraction imports
   - Extraction calls (fonctions appel√©es)
   - Extraction decorators
   - D√©tection tests (has_tests, test_files)

2. ‚úÖ Ajouter d√©pendance `radon`
   ```
   radon>=6.0.1  # Complexit√© Python
   ```

3. ‚úÖ Cr√©er mod√®le `CodeMetadata` (Pydantic)
   ```python
   class CodeMetadata(BaseModel):
       language: str
       chunk_type: str
       name: str
       signature: Optional[str]
       parameters: list[str]
       returns: Optional[str]
       docstring: Optional[str]
       complexity: dict  # {cyclomatic, lines_of_code, cognitive}
       imports: list[str]
       calls: list[str]
       decorators: list[str]
       has_tests: bool
       test_files: list[str]
       last_modified: Optional[datetime]
   ```

4. ‚úÖ Tests unitaires
   - Test Python function metadata
   - Test JavaScript class metadata
   - Test complexity calculation
   - Test docstring parsing (Google style)
   - Test decorator extraction

**Acceptance Criteria**:
- ‚úÖ M√©tadonn√©es extraites sur >90% fonctions Python
- ‚úÖ Performance <20ms par chunk
- ‚úÖ Tests coverage >85%
- ‚úÖ Support multi-langages (Python prioritaire)

**D√©pendances**:
- Story 1 (chunking)
- radon 6.0+

---

### **PHASE 2: Graph Intelligence (3 semaines)**

**Objectif**: Call graph + dependency analysis

**Story Points**: 13

#### Story 4: Dependency Graph Construction (13 points)

**Complexit√©**: HAUTE
**Dur√©e estim√©e**: 3 semaines

**Tasks**:
1. ‚úÖ Impl√©menter `GraphAnalysisService`
   - Static analysis call graph (Python via `ast`)
   - Import graph
   - Extraction data flow (variable usages)

2. ‚úÖ Extension tables `nodes` / `edges`
   - `ALTER TABLE nodes ADD COLUMN code_metadata JSONB`
   - `ALTER TABLE edges ADD COLUMN call_frequency INT DEFAULT 0`

3. ‚úÖ Repository `GraphRepository`
   - `create_node()`
   - `create_edge()`
   - `get_graph(from_node, relationship, direction, depth)`
   - Requ√™tes CTE r√©cursives (‚â§3 hops)

4. ‚úÖ API endpoints
   - `GET /v1/code/graph?from=<uuid>&relationship=calls&direction=outbound&depth=2`

5. ‚úÖ Tests
   - Test call graph Python (~500 functions)
   - Test import graph
   - Test CTE r√©cursifs (depth 1, 2, 3)
   - Test visualisation JSON

**Acceptance Criteria**:
- ‚úÖ Call graph extraction op√©rationnel
- ‚úÖ CTE r√©cursifs ‚â§3 hops
- ‚úÖ API endpoint fonctionnel
- ‚úÖ Tests avec codebase r√©elle

**D√©pendances**:
- Story 2bis (code_chunks table)
- Tables nodes/edges existantes

**R√©f√©rences**:
- Python `ast` module (stdlib)
- Tree-sitter queries pour autres langages

---

### **PHASE 3: Hybrid Search (3 semaines)**

**Objectif**: pg_trgm + Vector + RRF fusion

**Story Points**: 21

#### Story 5: Hybrid Search (BM25-like + Vector + Graph) (21 points)

**Complexit√©**: HAUTE
**Dur√©e estim√©e**: 3 semaines

**Tasks**:
1. ‚úÖ Setup pg_trgm
   ```sql
   CREATE EXTENSION IF NOT EXISTS pg_trgm;
   CREATE INDEX idx_code_source_trgm ON code_chunks USING gin (source_code gin_trgm_ops);
   ```

2. ‚úÖ Impl√©menter `HybridCodeSearchService`
   - `_pg_trgm_search()` (lexical similarity)
   - `_vector_search()` (semantic via HNSW)
   - `_rrf_fusion()` (Reciprocal Rank Fusion, k=60)
   - `_expand_with_graph()` (optionnel, depth 0-3)

3. ‚úÖ M√©thode RRF
   ```python
   def _rrf_fusion(self, list1, list2, k=60) -> list:
       """
       RRF score = 1 / (rank + k)
       Combine scores from multiple result lists.
       """
       scores = {}
       for rank, item in enumerate(list1, 1):
           scores[item.id] = scores.get(item.id, 0) + 1 / (rank + k)
       for rank, item in enumerate(list2, 1):
           scores[item.id] = scores.get(item.id, 0) + 1 / (rank + k)

       sorted_items = sorted(scores.items(), key=lambda x: x[1], reverse=True)
       return [self._get_item(id) for id, score in sorted_items]
   ```

4. ‚úÖ API endpoint
   - `POST /v1/code/search`
   - Param√®tres: query, language, chunk_type, use_trgm, use_vector, use_graph, limit

5. ‚úÖ Benchmark
   - Recall pg_trgm vs vector vs hybrid
   - Latency P95 (<50ms target)

6. ‚úÖ Tests
   - Test pg_trgm search
   - Test vector search
   - Test RRF fusion
   - Test graph expansion
   - Test end-to-end hybrid

**Acceptance Criteria**:
- ‚úÖ pg_trgm search op√©rationnel
- ‚úÖ Vector search via HNSW
- ‚úÖ RRF fusion impl√©ment√©
- ‚úÖ Graph expansion optionnel
- ‚úÖ Benchmark recall +X% vs vector-only
- ‚úÖ Latency P95 <50ms (10k chunks)

**D√©pendances**:
- Story 2bis (code_chunks table)
- Story 4 (graph)
- pg_trgm extension

**‚ö†Ô∏è NOTE CRITIQUE**: Pas de vrai BM25, utiliser pg_trgm similarity comme proxy lexical. √âvaluer extensions BM25 (pg_search, VectorChord) post-v1.4.0 si qualit√© insuffisante.

**R√©f√©rences**:
- https://jkatz05.com/post/postgres/hybrid-search-postgres-pgvector/
- RRF algorithm: 1/(rank+k)

---

### **PHASE 4: API & Integration (2 semaines)**

**Objectif**: Pipeline complet + documentation

**Story Points**: 13

#### Story 6: Code Indexing Pipeline & API (13 points)

**Complexit√©**: HAUTE
**Dur√©e estim√©e**: 2 semaines

**Tasks**:
1. ‚úÖ Impl√©menter `CodeIndexingService`
   - Pipeline: Language Detection ‚Üí AST Parsing ‚Üí Chunking ‚Üí Metadata Extraction ‚Üí Dependency Analysis ‚Üí Embedding Generation ‚Üí DB Storage
   - Batch processing (plusieurs fichiers)
   - Error handling robuste
   - Progress tracking

2. ‚úÖ API endpoints
   ```python
   # POST /v1/code/index
   {
     "repository": "my-project",
     "files": [
       {"path": "src/main.py", "content": "..."},
       {"path": "src/utils.py", "content": "..."}
     ],
     "options": {
       "language": "python",
       "analyze_dependencies": true,
       "extract_metadata": true,
       "generate_embeddings": true
     }
   }

   # Response
   {
     "indexed_chunks": 127,
     "indexed_nodes": 89,
     "indexed_edges": 243,
     "processing_time_ms": 4523,
     "repository_id": "<uuid>"
   }
   ```

3. ‚úÖ Documentation OpenAPI
   - `/v1/code/index` (POST)
   - `/v1/code/search` (GET)
   - `/v1/code/graph` (GET)

4. ‚úÖ Tests end-to-end
   - Test indexing repo r√©el (~100 fichiers Python)
   - Test error handling (fichiers invalides)
   - Test rate limiting

5. ‚úÖ UI v4.0 integration
   - Page code search
   - Page graph visualization

**Acceptance Criteria**:
- ‚úÖ Endpoint `/v1/code/index` op√©rationnel
- ‚úÖ Support batch indexing
- ‚úÖ Error handling robuste
- ‚úÖ Documentation OpenAPI compl√®te
- ‚úÖ Tests end-to-end passent

**D√©pendances**:
- Stories 1-5 compl√©t√©es

---

## üìä M√©triques de Succ√®s

### Performance

| M√©trique | Baseline | Target Phase 4 | Mesure |
|----------|----------|----------------|---------|
| **Indexing** | N/A | <500ms/file (300 LOC) | P95 |
| **Search hybrid** | N/A | <50ms | P95 (10k chunks) |
| **Graph traversal** | N/A | <20ms (depth=2) | P95 |
| **RAM embeddings** | 262 MB | <1 GB | Total (nomic-text + jina-code) |

### Qualit√©

| M√©trique | Target | Mesure |
|----------|--------|--------|
| **Code chunking quality** | >80% fonctions compl√®tes | % chunks s√©mantiques valides |
| **Search recall** | >85% | Recall@10 sur CodeSearchNet |
| **Metadata extraction** | >90% | % fonctions avec metadata compl√®te |
| **Tests coverage** | >85% | pytest-cov sur nouveaux modules |

### Backward Compatibility

| M√©trique | Target | Validation |
|----------|--------|------------|
| **Events API** | 0 breaking changes | Tests regression v1 API |
| **Events search** | Performance maintenue | Latency ‚â§ baseline |
| **Database migrations** | R√©versibles | Alembic downgrade fonctionne |

---

## üöß Risques & Mitigations

| Risque | Probabilit√© | Impact | Mitigation |
|--------|-------------|--------|------------|
| **Tree-sitter complexit√©** | Moyen | Haut | PoC Python d'abord, fallback chunking fixe |
| **Dual embeddings RAM overflow** | Faible | Moyen | Monitoring RAM, quantization si besoin |
| **pg_trgm insuffisant** | Moyen | Moyen | Benchmark vs vector, √©valuer pg_search extension |
| **Graph traversal lent** | Moyen | Moyen | Limiter depth ‚â§3, index sur edges, CTE optimis√©s |
| **Breaking changes accidentels** | Faible | Haut | Tests regression automatiques, review PRs |
| **Scope creep** | Haut | Moyen | Stick to 6 stories, reporter features v1.5.0 |

---

## ‚úÖ Checklist Pre-Implementation

### Infrastructure
- [ ] Alembic initialis√© avec template async
- [ ] Dual embeddings service test√© (RAM < 1 GB)
- [ ] Variables environnement configur√©es
- [ ] Extensions PostgreSQL v√©rifi√©es (pgvector, pg_trgm)

### Code Quality
- [ ] Tests coverage target d√©fini (>85%)
- [ ] Linters configur√©s (ruff, mypy)
- [ ] Pre-commit hooks actifs
- [ ] CI/CD pipeline pr√™t

### Documentation
- [ ] README mis √† jour (section Code Intelligence)
- [ ] API docs OpenAPI pr√™tes
- [ ] ADRs r√©dig√©es (tree-sitter, jina-code, pg_trgm)

### Team Alignment
- [ ] Plan valid√© par stakeholders
- [ ] Ressources allou√©es (13 semaines)
- [ ] Priorit√©s claires (backward compatibility critical)

---

## üìö R√©f√©rences Cl√©s

### Papers & Research
- cAST: Chunking via Abstract Syntax Trees (arxiv 2024, ICLR 2025)
- jina-embeddings-v2-base-code (Jina AI, Lead 9/15 CodeSearchNet)
- Hybrid Search with PostgreSQL and pgvector (Jonathan Katz, 2024)

### Libraries & Tools
- tree-sitter-languages: https://pypi.org/project/tree-sitter-languages/
- sentence-transformers: https://huggingface.co/sentence-transformers
- radon: https://github.com/rubik/radon
- alembic: https://alembic.sqlalchemy.org/

### PostgreSQL Extensions
- pgvector: https://github.com/pgvector/pgvector
- pg_trgm: https://www.postgresql.org/docs/current/pgtrgm.html
- (√âvaluation future) pg_search: https://blog.paradedb.com/pages/introducing_search

---

**Date**: 2025-10-15
**Version**: 1.0.0
**Statut**: ‚úÖ PLAN VALID√â - PR√äT POUR KICKOFF PHASE 0

**Prochaine Action**: Validation stakeholders ‚Üí Kickoff Phase 0 Story 0.1 (Alembic Setup)
