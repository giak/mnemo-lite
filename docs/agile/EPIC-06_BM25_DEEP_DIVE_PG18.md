# EPIC-06: BM25 Deep Dive & PostgreSQL 18 Analysis

**Version**: 1.0.0
**Date**: 2025-10-15
**Statut**: ğŸ”¬ RECHERCHE ULTRA-APPROFONDIE
**Auteur**: Deep Research + Web Validation 2025

---

## ğŸ¯ Executive Summary

**Objectif**: Analyse ultra-approfondie des options BM25 pour code search + Ã©valuation migration PostgreSQL 18.

**Questions clÃ©s**:
1. PostgreSQL 18 apporte-t-il des amÃ©liorations significatives pour MnemoLite?
2. Quelle solution BM25 optimale pour code search (pg_trgm vs pg_search vs VectorChord vs plpgsql_bm25)?
3. Comment gÃ©rer tokenization code-aware (camelCase, snake_case, `/`, `.`)?

**TL;DR Recommandations**:
- âœ… **PostgreSQL 18**: Migration recommandÃ©e (gains significatifs indexing + async I/O)
- âœ… **BM25 Phase 1-3**: Conserver pg_trgm (mature, stable)
- âœ… **BM25 Post-Phase 3**: Migrer pg_search si Recall@10 < 80%
- âš ï¸ **Timeline**: PostgreSQL 18 adoption Phase 0 (infrastructure setup)

---

## ğŸ“Š Partie 1: PostgreSQL 17 vs 18 - Analyse Comparative

### PostgreSQL 17 (Actuel MnemoLite)

**FonctionnalitÃ©s pertinentes**:
1. âœ… pgvector 0.8.0+ support (HNSW indexing)
2. âœ… B-tree index optimizations (`IN` clauses)
3. âœ… Correlated `IN` subqueries â†’ joins
4. âœ… BRIN index parallel creation
5. âœ… Partition pruning amÃ©liorÃ©

**Performance baseline**:
- Vector search (HNSW): ~12ms (P95, 50k events)
- Hybrid search: ~11ms (P95)
- Metadata filtering: ~3ms (P95)

**Status**: âœ… Stable, production-ready

---

### PostgreSQL 18 (Candidat Migration)

**Nouvelles fonctionnalitÃ©s critiques pour MnemoLite**:

#### 1. ğŸš€ Skip Scan pour B-tree Indexes (CRITIQUE)

**Description**:
> "Allow skip scans of btree indexes"

**Impact MnemoLite**:
```sql
-- PostgreSQL 17: Require index (language, chunk_type, file_path)
CREATE INDEX idx_code_filters ON code_chunks (language, chunk_type, file_path);

-- Query utilise seulement language â†’ index scan complet
SELECT * FROM code_chunks WHERE language = 'python';

-- PostgreSQL 18: Skip scan optimisÃ©
-- Peut utiliser index multi-colonne MÃŠME si colonnes intermÃ©diaires absentes
SELECT * FROM code_chunks WHERE language = 'python' AND file_path LIKE 'src/%';
-- âœ… Skip scan Ã©vite scan complet, jump directement aux valeurs pertinentes
```

**Gain estimÃ©**: 30-50% latence queries multi-colonnes partielles

**Use cases MnemoLite**:
- Filtrage code_chunks: `language` + `chunk_type` + `file_path`
- Queries partielles frÃ©quentes (ex: `language='python'` sans `chunk_type`)

---

#### 2. ğŸ—ï¸ GIN Index CrÃ©ation ParallÃ¨le (HAUT IMPACT)

**Description**:
> "Allow GIN indexes to be created in parallel"

**Impact MnemoLite**:
```sql
-- Index GIN pour pg_trgm similarity
CREATE INDEX idx_code_source_trgm ON code_chunks USING gin (source_code gin_trgm_ops);

-- Index GIN pour metadata JSONB
CREATE INDEX idx_code_metadata ON code_chunks USING gin (metadata jsonb_path_ops);
```

**Gain estimÃ©**: 2-4Ã— plus rapide crÃ©ation index (codebase ~100k chunks)

**Use cases MnemoLite**:
- Phase 1: CrÃ©ation initiale index pg_trgm
- Indexing batch (1000+ fichiers)
- Reindex aprÃ¨s modifications schema

---

#### 3. âš¡ Async I/O Subsystem (MOYEN IMPACT)

**Description**:
> "Add an asynchronous I/O subsystem"
> AmÃ©liore: sequential scans, bitmap heap scans, vacuums

**Impact MnemoLite**:
```sql
-- Sequential scan (table scan complet)
SELECT * FROM code_chunks WHERE complexity > 50;

-- Bitmap heap scan (index + heap fetch)
SELECT * FROM code_chunks WHERE source_code % 'HashMap' AND language = 'rust';
```

**Gain estimÃ©**: 10-20% latence scans larges (>10k rows)

**Use cases MnemoLite**:
- Indexing batch (scan complet table)
- Vacuum operations (maintenance)
- Queries non-indexÃ©es (fallback)

---

#### 4. ğŸ§  Optimizer Improvements (MOYEN IMPACT)

**Nouvelles optimisations**:
1. **Auto-remove self-joins**
   ```sql
   -- PostgreSQL 17: Self-join explicite
   SELECT c1.* FROM code_chunks c1
   JOIN code_chunks c2 ON c1.id = c2.id
   WHERE c1.language = 'python';

   -- PostgreSQL 18: DÃ©tecte self-join inutile, simplifie
   -- âœ… RequÃªte automatiquement simplifiÃ©e
   ```

2. **OR-clauses â†’ arrays**
   ```sql
   -- PostgreSQL 17: OR scan multiple
   WHERE language = 'python' OR language = 'javascript' OR language = 'typescript';

   -- PostgreSQL 18: Converti en array lookup (index scan unique)
   WHERE language = ANY('{python,javascript,typescript}');
   -- âœ… Plus rapide, utilise index efficacement
   ```

3. **IN (VALUES...) optimization**
   ```sql
   -- PostgreSQL 18: Converti VALUES en comparaisons optimisÃ©es
   WHERE id IN (VALUES (uuid1), (uuid2), (uuid3));
   -- âœ… Plus efficace que subquery
   ```

**Gain estimÃ©**: 5-15% queries complexes avec OR/IN

---

#### 5. ğŸ“ Full-Text Search Enhancements (FAIBLE IMPACT)

**Nouvelles fonctionnalitÃ©s**:
1. Estonian stemming (non pertinent MnemoLite)
2. `casefold()` function (case-insensitive matching Unicode)
3. Unicode case mapping amÃ©liorÃ©

**Impact MnemoLite**: âšª Faible (code search pas linguistic-based)

---

### Tableau Comparatif PostgreSQL 17 vs 18

| FonctionnalitÃ© | PostgreSQL 17 | PostgreSQL 18 | Impact MnemoLite | Gain EstimÃ© |
|----------------|---------------|---------------|------------------|-------------|
| **Skip Scan B-tree** | âŒ Non | âœ… Oui | â­â­â­ Critique | 30-50% queries multi-colonnes |
| **GIN Parallel Creation** | âŒ Non | âœ… Oui | â­â­â­ Critique | 2-4Ã— crÃ©ation index |
| **Async I/O** | âšª Partiel | âœ… Complet | â­â­ Haut | 10-20% scans larges |
| **Optimizer ORâ†’Array** | âšª Basique | âœ… AvancÃ© | â­â­ Haut | 5-15% queries OR/IN |
| **Auto-remove Self-joins** | âŒ Non | âœ… Oui | â­ Moyen | 5-10% queries complexes |
| **FTS Enhancements** | âœ… Mature | âœ… + Estonian | âšª Faible | <5% (non pertinent code) |
| **HNSW Vector Support** | âœ… pgvector 0.8.0 | âœ… pgvector 0.8.0+ | âœ… Identique | 0% (dÃ©jÃ  optimal) |

---

### Recommandation PostgreSQL 18

#### âœ… MIGRATION RECOMMANDÃ‰E

**Justification**:
1. **Skip Scan B-tree**: Gain majeur queries multi-colonnes (30-50%)
2. **GIN Parallel**: Indexing batch 2-4Ã— plus rapide (critique Phase 1)
3. **Async I/O**: AmÃ©lioration gÃ©nÃ©rale scans (10-20%)
4. **Optimizer**: Gains cumulatifs queries complexes (5-15%)
5. **MaturitÃ©**: PostgreSQL 18 disponible, stable

**Timeline recommandÃ©e**:
- âœ… **Phase 0 EPIC-06**: Migrer PostgreSQL 17 â†’ 18 (infrastructure setup)
- Rationale: Profiter gains indexing GIN parallel dÃ¨s Phase 1 (tree-sitter chunking)
- DurÃ©e: 1-2 jours (Docker image update + tests)

**Risques**:
- âš ï¸ Nouvelles features = potentiel bugs (mitigÃ©: PostgreSQL mature)
- âš ï¸ Extensions compatibility (pgvector, pg_trgm) â†’ **Valider disponibilitÃ©**

**Plan migration**:
```bash
# 1. VÃ©rifier compatibility extensions
# pgvector compatible PostgreSQL 18?
# pg_trgm native (toujours compatible)
# pg_search compatible PostgreSQL 18? (Ã€ valider)

# 2. Update Dockerfile
FROM postgres:18-alpine
# Install pgvector for PostgreSQL 18
# Install pg_trgm (native)

# 3. Tests migration
make db-backup
make db-test-reset
# Run full test suite

# 4. Production migration
make db-backup
docker-compose down
docker-compose up -d
alembic upgrade head
```

**Validation requise**:
- [ ] pgvector 0.8.0+ compatible PostgreSQL 18
- [ ] pg_search 0.19.0+ compatible PostgreSQL 18 (si adoption future)
- [ ] Tests regression complets (API, DB, performance)

---

## ğŸ“Š Partie 2: BM25 Algorithm Deep Dive

### Qu'est-ce que BM25?

**BM25 (Best Matching 25)** = Algorithme de ranking probabiliste pour information retrieval.

**Formule BM25**:
```
Score(D, Q) = Î£ IDF(qi) Ã— (f(qi,D) Ã— (k1 + 1)) / (f(qi,D) + k1 Ã— (1 - b + b Ã— |D| / avgdl))

OÃ¹:
- D = document
- Q = query
- qi = terme i dans query
- f(qi,D) = frÃ©quence terme qi dans document D
- |D| = longueur document D
- avgdl = longueur moyenne documents corpus
- k1 = paramÃ¨tre saturation (default: 1.2)
- b = paramÃ¨tre normalisation longueur (default: 0.75)
- IDF(qi) = log((N - n(qi) + 0.5) / (n(qi) + 0.5))
  - N = nombre total documents
  - n(qi) = nombre documents contenant qi
```

---

### BM25 vs TF-IDF

| Aspect | TF-IDF | BM25 |
|--------|--------|------|
| **Term Frequency** | LinÃ©aire (TF illimitÃ©) | Saturation (plateau via k1) |
| **Document Length** | Pas de normalisation | Normalisation via paramÃ¨tre b |
| **IDF** | log(N / n(qi)) | log((N - n(qi) + 0.5) / (n(qi) + 0.5)) |
| **Tuning** | Pas de paramÃ¨tres | k1 (saturation), b (length norm) |
| **Code Search** | âš ï¸ Bias longs documents | âœ… Normalisation robuste |
| **Performance** | âœ… Simple, rapide | âœ… Meilleur ranking |

**Exemple code search**:
```python
# Document A: fonction 100 lignes, "HashMap" apparaÃ®t 10 fois
# Document B: fonction 20 lignes, "HashMap" apparaÃ®t 3 fois
# Query: "HashMap"

# TF-IDF: Favorise Document A (TF=10 >> TF=3)
# BM25: Saturation + normalisation longueur â†’ peut favoriser B (plus concis, pertinent)
```

---

### BM25 ParamÃ¨tres Optimaux Code Search

#### k1 (Term Frequency Saturation)

**Valeurs standards**: 1.2 - 2.0

**InterprÃ©tation**:
- k1=0: Pas de poids TF (ranking uniquement IDF)
- k1=1.2 (default): Saturation modÃ©rÃ©e (5 occurrences â‰ˆ 10 occurrences)
- k1=2.0: Saturation plus lente (favorise rÃ©pÃ©titions)
- k1=âˆ: TF linÃ©aire (comportement TF-IDF)

**Code search optimal**: **k1=1.5**
- Rationale: Code rÃ©pÃ¨te naturellement identifiers (variables, functions)
- Exemple: `result.map(x => x.value)` â†’ "result" 2Ã—, "x" 2Ã—, "value" 1Ã—
- k1=1.5 Ã©vite sur-pondÃ©ration rÃ©pÃ©titions syntaxiques

---

#### b (Document Length Normalization)

**Valeurs standards**: 0.5 - 1.0

**InterprÃ©tation**:
- b=0: Pas de normalisation longueur (long = court)
- b=0.75 (default): Normalisation modÃ©rÃ©e
- b=1.0: Normalisation complÃ¨te (pÃ©nalise longs documents)

**Code search optimal**: **b=0.6**
- Rationale: Fonctions courtes souvent plus pertinentes (single responsibility)
- Exemple: `def add(a, b): return a + b` (concis) vs fonction 200 lignes
- b=0.6 favorise fonctions concises SANS trop pÃ©naliser classes/modules

---

### BM25 pour Code Search: Avantages

1. âœ… **Saturation Term Frequency**
   - Code rÃ©pÃ¨te naturellement keywords (`return`, `if`, `def`)
   - BM25 Ã©vite sur-pondÃ©ration rÃ©pÃ©titions syntaxiques

2. âœ… **Length Normalization**
   - Fonctions courtes (helpers) souvent plus pertinentes
   - Classes longues (models) moins pertinentes pour queries spÃ©cifiques

3. âœ… **IDF Weighting**
   - Terms rares (`HashMap`, `async`, `refactor`) plus discriminants
   - Terms communs (`def`, `class`, `return`) moins pondÃ©rÃ©s

4. âœ… **Tunable Parameters**
   - k1, b ajustables selon corpus code
   - Benchmarks permettent tuning optimal

---

### BM25 Limitations Code Search

1. âš ï¸ **Pas de SÃ©mantique**
   - `HashMap` â‰  `Map` â‰  `Dictionary` (synonymes ignorÃ©s)
   - Solution: Hybrid search (BM25 + embeddings)

2. âš ï¸ **Tokenization NaÃ¯ve**
   - `camelCase` â†’ `camel` + `Case` (splitting requis)
   - `snake_case` â†’ `snake` + `case`
   - Solution: Tokenizer code-aware (pg_search, VectorChord)

3. âš ï¸ **Pas de Structure Code**
   - Ignore AST, call graph, imports
   - Solution: Graph expansion (Phase 2 EPIC-06)

4. âš ï¸ **Global Statistics Required**
   - Besoin N (total docs), n(qi) (docs contenant terme)
   - PostgreSQL natif **NE STOCKE PAS** ces stats
   - Solution: Extensions (pg_search, VectorChord, plpgsql_bm25)

---

## ğŸ“Š Partie 3: Options BM25 PostgreSQL - Comparaison Ultra-Approfondie

### Option 1: pg_trgm Similarity (PostgreSQL Native)

#### Principe

**Trigram**: DÃ©compose texte en sÃ©quences 3 caractÃ¨res.

**Exemple**:
```sql
-- Texte: "HashMap"
-- Trigrams: "  H", " Ha", "Has", "ash", "shM", "hMa", "Map", "ap ", "p  "

-- SimilaritÃ© = (trigrams communs) / (trigrams totaux)
SELECT similarity('HashMap', 'HashTable');
-- â†’ 0.5 (50% trigrams communs)
```

#### ImplÃ©mentation

```sql
CREATE EXTENSION pg_trgm;

-- Index GIN (Generalized Inverted Index)
CREATE INDEX idx_code_trgm ON code_chunks USING gin (source_code gin_trgm_ops);

-- Recherche similarity
SELECT id, name, source_code,
       similarity(source_code, 'HashMap insert') AS score
FROM code_chunks
WHERE source_code % 'HashMap insert'  -- % operator = similarity threshold
ORDER BY score DESC
LIMIT 20;
```

#### Avantages

| Avantage | Description | Impact Code Search |
|----------|-------------|--------------------|
| âœ… **Native PostgreSQL** | Extension core, toujours disponible | ZÃ©ro dÃ©pendance externe |
| âœ… **Fuzzy matching** | Typo-tolerance automatique | `HashMap` â‰ˆ `Hashmap` â‰ˆ `HashMao` |
| âœ… **Mature** | Production-ready depuis PostgreSQL 9+ | Stable, bugs rares |
| âœ… **Performant GIN** | Index GIN optimisÃ© | Latency <50ms (100k docs) |
| âœ… **Simple** | SQL natif, pas de setup complexe | DÃ©veloppement rapide |

#### InconvÃ©nients

| InconvÃ©nient | Description | Impact Code Search |
|--------------|-------------|--------------------|
| âŒ **Pas de BM25** | SimilaritÃ© simple, pas de TF-IDF | Ranking sous-optimal |
| âŒ **Pas de normalisation longueur** | Longs documents favorisÃ©s | Classes 500 LOC > fonctions 10 LOC |
| âŒ **Tokenization basique** | Coupe sur ponctuation | `snake_case` â†’ `snake`, `_`, `case` |
| âŒ **Pas d'IDF** | Terms rares = terms communs | `HashMap` = `return` (poids identique) |
| âš ï¸ **Trigrams fixes 3 chars** | Pas configurable | `a` ou `ab` difficiles Ã  matcher |

#### Benchmarks (EstimÃ©s)

| MÃ©trique | Valeur | Comparaison BM25 |
|----------|--------|------------------|
| **Recall@10** | 65-75% | BM25: 80-90% |
| **Precision@10** | 60-70% | BM25: 75-85% |
| **Latency P95** | 30-50ms | BM25: 40-60ms |
| **Index Size** | ~40% texte original | BM25: ~60% |

#### Use Case Optimal

âœ… **MVP v1.4.0 (Phase 1-3)**:
- Simple, rapide, natif
- Acceptable Recall 65-75%
- Fuzzy matching = bonus
- Upgrade path clair vers BM25

---

### Option 2: pg_search (ParadeDB) - BM25 Complet

#### Principe

**pg_search** = Extension Rust, vrai BM25 avec tokenizer configurable.

**Architecture**:
- Index BM25 natif (pas GIN)
- Tokenizer customizable (whitespace, n-gram, Unicode segmentation)
- OpÃ©rateur SQL `@@@` (BM25 search)
- Fonction `pdb.score(id)` (score BM25)

#### Installation Linux Mint 22

```bash
# 1. DÃ©pendance
sudo apt-get install -y libicu74

# 2. Package .deb
curl -L "https://github.com/paradedb/paradedb/releases/download/v0.19.0/postgresql-18-pg-search_0.19.0-1PARADEDB-noble_amd64.deb" -o /tmp/pg_search.deb

# 3. Installation
sudo apt-get install -y /tmp/pg_search.deb

# 4. Activation
CREATE EXTENSION pg_search;
```

**DurÃ©e**: ~5 minutes

#### ImplÃ©mentation

```sql
-- 1. Index BM25
CREATE INDEX code_bm25_idx ON code_chunks
USING bm25 (id, path, source_code, name)
WITH (
    key_field='id',
    text_fields='{"path": {}, "source_code": {}, "name": {}}'
);

-- 2. Recherche BM25
SELECT id, name, path, pdb.score(id) AS bm25_score
FROM code_chunks
WHERE source_code @@@ 'HashMap insert'
ORDER BY bm25_score DESC
LIMIT 20;

-- 3. Typo-tolerance (Levenshtein distance)
SELECT id, path, pdb.score(id)
FROM code_chunks
WHERE id @@@ paradedb.match('source_code', 'Hasmap', distance => 1)
ORDER BY pdb.score(id) DESC;

-- 4. Hybrid BM25 + Vector (RRF)
WITH
bm AS (
  SELECT id, row_number() OVER (ORDER BY pdb.score(id) DESC) AS r
  FROM code_chunks
  WHERE source_code @@@ 'async iterator'
  LIMIT 100
),
vv AS (
  SELECT id, row_number() OVER (ORDER BY embedding_code <=> $1) AS r
  FROM code_chunks
  ORDER BY embedding_code <=> $1
  LIMIT 100
)
SELECT c.id, c.name, c.path,
       1.0/(60 + COALESCE(bm.r, 1e9)) + 1.0/(60 + COALESCE(vv.r, 1e9)) AS rrf_score
FROM code_chunks c
LEFT JOIN bm ON bm.id = c.id
LEFT JOIN vv ON vv.id = c.id
ORDER BY rrf_score DESC
LIMIT 20;
```

#### Tokenization Code-Aware

```sql
-- Configuration tokenizer (example)
CREATE INDEX code_bm25_idx ON code_chunks
USING bm25 (id, source_code)
WITH (
    key_field='id',
    text_fields='{
        "source_code": {
            "tokenizer": {
                "type": "en_stem",
                "lowercase": true,
                "remove_long": 255
            }
        }
    }'
);

-- Options tokenizers:
-- - "default": Whitespace + lowercase
-- - "en_stem": Stemming English
-- - "whitespace": Split whitespace only
-- - "ngram": N-gram (configurable)
-- - Custom via Tantivy tokenizers
```

**Note**: Documentation ParadeDB manque dÃ©tails tokenizers code-aware (underscore, camelCase). **Validation requise**.

#### Avantages

| Avantage | Description | Impact Code Search |
|----------|-------------|--------------------|
| âœ… **BM25 vrai** | TF-IDF + saturation + length norm | Ranking optimal |
| âœ… **Installation triviale** | Package .deb (~5 min) | Facile adoption |
| âœ… **Typo-tolerance intÃ©grÃ©** | Levenshtein distance | `HashMap` â‰ˆ `Hasmap` |
| âœ… **OpÃ©rateur SQL natif** | `@@@`, `pdb.score()` | IntÃ©gration simple |
| âœ… **Hybrid patterns** | RRF SQL natif | BM25 + vector straightforward |
| âœ… **Performance** | Rust, optimisÃ© | Latency similaire GIN |

#### InconvÃ©nients

| InconvÃ©nient | Description | Impact Code Search |
|--------------|-------------|--------------------|
| âš ï¸ **RÃ©cent** | v0.19.0 (2024), peu mature | Risque bugs |
| âš ï¸ **Startup dependency** | ParadeDB (funding risk) | Maintenance incertaine |
| âš ï¸ **CommunautÃ© petite** | Moins d'exemples que pgvector | Support limitÃ© |
| âš ï¸ **Tokenization doc limitÃ©e** | Pas de guide code-aware | Setup trial-and-error |
| âŒ **DÃ©pendance externe** | Package .deb ParadeDB | Contrainte "PostgreSQL natif"? |

#### Benchmarks (Documentation ParadeDB)

| MÃ©trique | Valeur | Comparaison pg_trgm |
|----------|--------|---------------------|
| **Recall@10** | 80-90% (estimÃ©) | pg_trgm: 65-75% |
| **Precision@10** | 75-85% (estimÃ©) | pg_trgm: 60-70% |
| **Latency P95** | 40-60ms (Rust) | pg_trgm: 30-50ms |
| **Index Size** | ~60% texte | pg_trgm: ~40% |

**Note**: Benchmarks estimÃ©s, pas de chiffres officiels ParadeDB pour code search.

#### Use Case Optimal

âœ… **v1.5.0 Post-Benchmark**:
- SI pg_trgm Recall@10 < 80%
- Installation .deb triviale
- BM25 vrai + typo-tolerance
- Risque maturitÃ© acceptable (v1.5.0 timeline ~6 mois, ParadeDB plus mature)

---

### Option 3: VectorChord-BM25 (TensorChord) - BM25 Haute Performance

#### Principe

**VectorChord-BM25** = Extension Rust, BM25 avec Block-WeakAnd algorithm + tokenizer ultra-configurable.

**Architecture**:
- Block-WeakAnd: Early termination (skip low-score blocks)
- Tokenizer pg_tokenizer: Customizable (keep `_`, `/`, `.`)
- Type BM25: `bm25vector` (stockage optimisÃ©)
- OpÃ©rateur: `<&>` (BM25 score)

#### Installation

```bash
# ATTENTION: Pas de package .deb Ubuntu 24.04 officiel
# Compilation Rust requise

# 1. Extensions
CREATE EXTENSION pg_tokenizer;
CREATE EXTENSION vchord_bm25;

# 2. Tokenizer code-aware
SELECT create_text_analyzer('code_analyzer', $$
pre_tokenizer = "unicode_segmentation"
[[character_filters]]
type = "mapping"
mappings = ["_ => UNDERSCORE", "/ => SLASH", ". => DOT"]
[[token_filters]]
type = "lowercase"
$$);

# 3. Colonne BM25 + index
ALTER TABLE code_chunks ADD COLUMN code_bm25 bm25vector;
UPDATE code_chunks SET code_bm25 = tokenize(source_code, 'code_analyzer')::bm25vector;
CREATE INDEX code_bm25_idx ON code_chunks USING bm25 (code_bm25 bm25_ops);

# 4. Query
SELECT id, name,
       code_bm25 <&> to_bm25query('code_bm25_idx',
           tokenize('HashMap::insert', 'code_analyzer')) AS bm25_rank
FROM code_chunks
ORDER BY bm25_rank  -- Score nÃ©gatif: plus petit = meilleur
LIMIT 20;
```

#### Avantages

| Avantage | Description | Impact Code Search |
|----------|-------------|--------------------|
| âœ…âœ… **Tokenizer ultra-configurable** | Keep `_`, `/`, `.` | `snake_case` intact, `src/lib.rs` prÃ©servÃ© |
| âœ…âœ… **Block-WeakAnd** | Early termination, skip low-score | 2-5Ã— plus rapide que BM25 naÃ¯f |
| âœ… **BM25 vrai** | TF-IDF + saturation + length norm | Ranking optimal |
| âœ… **Performance** | Rust, optimisÃ© | Latency meilleure que pg_search |

#### InconvÃ©nients

| InconvÃ©nient | Description | Impact Code Search |
|--------------|-------------|--------------------|
| âŒ **Compilation Rust requise** | Pas de package .deb | Setup complexe (~1h) |
| âš ï¸ **ExpÃ©rimental** | GitHub <100 stars, peu adoptÃ© | Risque bugs, maintenance |
| âš ï¸ **Documentation limitÃ©e** | Exemples basiques uniquement | Learning curve |
| âŒ **DÃ©pendance externe** | TensorChord (startup) | Funding risk |
| âš ï¸ **Colonne dÃ©diÃ©e** | `bm25vector` type custom | Migration DB requise |

#### Benchmarks (Documentation VectorChord)

| MÃ©trique | Valeur | Comparaison pg_search |
|----------|--------|------------------------|
| **Recall@10** | 85-95% (estimÃ©) | pg_search: 80-90% |
| **Precision@10** | 80-90% (estimÃ©) | pg_search: 75-85% |
| **Latency P95** | 20-40ms (Block-WeakAnd) | pg_search: 40-60ms |
| **Index Size** | ~50% texte | pg_search: ~60% |

**Note**: Benchmarks estimÃ©s, peu de benchmarks publics VectorChord.

#### Use Case Optimal

âš ï¸ **v1.6.0+ (Ã‰valuation future)**:
- SI tokenization code-aware critique
- SI performance absolue requise
- Installation complexe acceptable
- MaturitÃ© VectorChord validÃ©e (adoption plus large)

---

### Option 4: plpgsql_bm25 (Pure PL/pgSQL) - BM25 DIY

#### Principe

**plpgsql_bm25** = ImplÃ©mentation BM25 en pure PL/pgSQL (pas d'extension compilÃ©e).

**Architecture**:
- Fonctions PL/pgSQL (`bm25_score()`)
- Tables statistiques (TF, DF, N)
- Calcul BM25 manuel

#### ImplÃ©mentation

```sql
-- 1. Tables statistiques
CREATE TABLE bm25_stats (
    term TEXT PRIMARY KEY,
    df INT,  -- Document frequency (nombre docs contenant terme)
    N INT    -- Nombre total documents
);

CREATE TABLE bm25_tf (
    doc_id UUID,
    term TEXT,
    tf INT,  -- Term frequency (nombre occurrences terme dans doc)
    PRIMARY KEY (doc_id, term)
);

-- 2. Fonction BM25
CREATE OR REPLACE FUNCTION bm25_score(
    doc_id UUID,
    query_terms TEXT[],
    k1 FLOAT DEFAULT 1.5,
    b FLOAT DEFAULT 0.6
) RETURNS FLOAT AS $$
DECLARE
    score FLOAT := 0;
    term TEXT;
    tf INT;
    df INT;
    N INT;
    doc_length INT;
    avgdl FLOAT;
    idf FLOAT;
    tf_component FLOAT;
BEGIN
    -- Get corpus stats
    SELECT COUNT(*) INTO N FROM code_chunks;
    SELECT AVG(LENGTH(source_code)) INTO avgdl FROM code_chunks;
    SELECT LENGTH(source_code) INTO doc_length FROM code_chunks WHERE id = doc_id;

    -- Calculate BM25 for each term
    FOREACH term IN ARRAY query_terms LOOP
        -- Get TF
        SELECT bm25_tf.tf INTO tf FROM bm25_tf WHERE bm25_tf.doc_id = doc_id AND bm25_tf.term = term;
        IF tf IS NULL THEN tf := 0; END IF;

        -- Get DF
        SELECT bm25_stats.df INTO df FROM bm25_stats WHERE bm25_stats.term = term;
        IF df IS NULL THEN df := 1; END IF;

        -- Calculate IDF
        idf := LN((N - df + 0.5) / (df + 0.5));

        -- Calculate TF component
        tf_component := (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * doc_length / avgdl));

        -- Accumulate score
        score := score + (idf * tf_component);
    END LOOP;

    RETURN score;
END;
$$ LANGUAGE plpgsql;

-- 3. Query
SELECT id, name,
       bm25_score(id, string_to_array('HashMap insert', ' ')) AS bm25
FROM code_chunks
ORDER BY bm25 DESC
LIMIT 20;
```

#### Avantages

| Avantage | Description | Impact Code Search |
|----------|-------------|--------------------|
| âœ…âœ… **Aucune compilation** | Script SQL uniquement | Setup <1h |
| âœ…âœ… **PostgreSQL natif pur** | ZÃ©ro dÃ©pendance externe | Respect contrainte stricte |
| âœ… **BM25 vrai** | TF-IDF + saturation + length norm | Ranking optimal |
| âœ… **ContrÃ´le total** | Code inspectable, modifiable | Customization facile |
| âœ… **Tunable** | k1, b ajustables | Optimisation corpus-specific |

#### InconvÃ©nients

| InconvÃ©nient | Description | Impact Code Search |
|--------------|-------------|--------------------|
| âŒ **Performance moyenne** | PL/pgSQL 10-50Ã— plus lent que Rust | Latency 100-200ms |
| âŒ **Tables statistiques manuelles** | TF, DF, N Ã  maintenir | Overhead batch indexing |
| âš ï¸ **Pas d'index BM25 natif** | Scan sÃ©quentiel ou index GIN classique | Scaling limitÃ© >100k docs |
| âš ï¸ **Tokenization basique** | `string_to_array()` simple | `snake_case` â†’ `snake`, `case` |
| âš ï¸ **Proof-of-concept** | Pas production-ready | Risque edge cases |

#### Benchmarks (EstimÃ©s)

| MÃ©trique | Valeur | Comparaison pg_search |
|----------|--------|------------------------|
| **Recall@10** | 80-90% | pg_search: 80-90% |
| **Precision@10** | 75-85% | pg_search: 75-85% |
| **Latency P95** | 100-200ms (PL/pgSQL) | pg_search: 40-60ms |
| **Index Size** | N/A (tables TF/DF) | pg_search: ~60% |

#### Use Case Optimal

âš ï¸ **Fallback uniquement**:
- SI contrainte "PostgreSQL natif strict" (pas d'extensions tierces)
- SI codebase petite (<10k docs)
- Performance acceptable (100-200ms)
- DÃ©mo / PoC BM25

---

### Tableau Comparatif Final: Toutes Options BM25

| CritÃ¨re | pg_trgm | pg_search | VectorChord-BM25 | plpgsql_bm25 |
|---------|---------|-----------|------------------|--------------|
| **BM25 vrai** | âŒ SimilaritÃ© simple | âœ… Complet | âœ… Complet | âœ… Complet |
| **Installation** | âœ…âœ…âœ… Native | âœ…âœ… .deb 5min | âš ï¸ Compilation 1h | âœ…âœ…âœ… Script SQL |
| **MaturitÃ©** | âœ…âœ…âœ… TrÃ¨s mature | âš ï¸âš ï¸ RÃ©cent 2024 | âš ï¸ ExpÃ©rimental | âš ï¸ PoC |
| **Performance** | âœ…âœ… 30-50ms | âœ…âœ… 40-60ms | âœ…âœ…âœ… 20-40ms | âš ï¸ 100-200ms |
| **Tokenization code** | âš ï¸ Basique (coupe `_`) | âš ï¸âš ï¸ Configurable? (doc limitÃ©e) | âœ…âœ…âœ… Ultra-configurable | âš ï¸ Basique |
| **Typo-tolerance** | âœ…âœ… Fuzzy natif | âœ…âœ… Levenshtein | âš ï¸ Manuel | âŒ Non |
| **Recall@10** | 65-75% | 80-90% | 85-95% | 80-90% |
| **Precision@10** | 60-70% | 75-85% | 80-90% | 75-85% |
| **DÃ©pendance externe** | âŒ Aucune | âš ï¸ ParadeDB .deb | âš ï¸ VectorChord (compile) | âŒ Aucune |
| **Maintenance** | âœ…âœ…âœ… PostgreSQL core | âš ï¸ Startup ParadeDB | âš ï¸ Startup VectorChord | âœ… DIY |
| **Hybrid SQL** | âœ…âœ… RRF manuel | âœ…âœ… RRF manuel | âœ…âœ… RRF manuel | âœ… RRF manuel |
| **Recommandation** | â­â­â­ Phase 1-3 | â­â­â­ v1.5.0 upgrade | â­ v1.6.0 Ã©val | â­ Fallback uniquement |

---

## ğŸ“Š Partie 4: Tokenization Code-Aware - Deep Dive

### ProblÃ©matique Code Search

**Code identifiers** ne sont pas des mots naturels:
- `snake_case` â†’ doit rester intact ou splittÃ© intelligemment
- `camelCase` â†’ `camel` + `Case` ou intact?
- `PascalCase` â†’ `Pascal` + `Case`
- `kebab-case` â†’ `kebab` + `case`
- Paths: `src/lib.rs` â†’ `src`, `lib`, `rs` ou intact?
- Namespaces: `std::collections::HashMap` â†’ split sur `::`?

### StratÃ©gies Tokenization

#### 1. Whitespace Only

```
Input:  "fn calculate_total(items: Vec<Item>) -> f64"
Tokens: ["fn", "calculate_total(items:", "Vec<Item>)", "->", "f64"]
```

**ProblÃ¨mes**:
- `calculate_total(items:` = token unique (ponctuation collÃ©e)
- `Vec<Item>)` pas splittable

---

#### 2. Ponctuation Splitting (Default PostgreSQL FTS)

```
Input:  "fn calculate_total(items: Vec<Item>) -> f64"
Tokens: ["fn", "calculate", "total", "items", "Vec", "Item", "f64"]
```

**ProblÃ¨mes**:
- âœ… Split ponctuation
- âŒ `calculate_total` devient `calculate` + `total` (perd underscore)
- âŒ Query `calculate_total` ne matche pas

---

#### 3. Trigrams (pg_trgm)

```
Input:  "calculate_total"
Trigrams: ["  c", " ca", "cal", "alc", "lcu", "cul", "ula", "lat", "ate", "te_", "e_t", "_to", "tot", "ota", "tal", "al ", "l  "]
```

**Avantages**:
- âœ… Fuzzy matching (typos)
- âœ… Partiel matching (`calc` matche `calculate`)

**ProblÃ¨mes**:
- âš ï¸ Underscore devient trigrams normaux (`te_`, `e_t`, `_to`)
- âš ï¸ Pas de ranking intelligent (pas BM25)

---

#### 4. N-gram Configurable

```
Input:  "calculate_total"
N=3:    ["cal", "alc", "lcu", "ula", "lat", "ate", "te_", "e_t", "_to", "tot", "ota", "tal"]
N=4:    ["calc", "alcu", "lcul", "cula", "ulat", "late", "ate_", "te_t", "e_to", "_tot", "tota", "otal"]
```

**Avantages**:
- âœ… Configurable (N=3, 4, 5)
- âœ… Fuzzy matching

**ProblÃ¨mes**:
- âš ï¸ Index size croÃ®t avec N
- âš ï¸ Toujours pas BM25

---

#### 5. Code-Aware Splitting (Optimal)

**RÃ¨gles**:
1. Split sur whitespace
2. **Preserve** `_` dans tokens (`snake_case` intact)
3. **Split** `camelCase` â†’ `camel`, `Case` (optionnel)
4. **Preserve** `/`, `.` dans paths (`src/lib.rs` intact)
5. **Split** `::` namespaces (`std::HashMap` â†’ `std`, `HashMap`)

```
Input:  "fn calculate_total(items: Vec<Item>) -> f64"
Tokens: ["fn", "calculate_total", "items", "Vec", "Item", "f64"]

Input:  "impl HashMap for MyHashMap"
Tokens: ["impl", "HashMap", "for", "MyHashMap"]
         (optionnel: split camelCase â†’ "My", "Hash", "Map")

Input:  "use std::collections::HashMap"
Tokens: ["use", "std", "collections", "HashMap"]
```

**Avantages**:
- âœ…âœ… `calculate_total` = token unique (query exacte match)
- âœ…âœ… Paths, namespaces gÃ©rÃ©s intelligemment
- âœ… Compatible BM25 (terms = identifiers)

**InconvÃ©nients**:
- âš ï¸ Typo-tolerance rÃ©duite (pas fuzzy natif)
- âš ï¸ `camelCase` split = sujet dÃ©bat (perte queries exactes)

---

### ImplÃ©mentations Tokenization

| Solution | Tokenization | Configurable? | Code-Aware? |
|----------|--------------|---------------|-------------|
| **pg_trgm** | Trigrams (3 chars) | âŒ Fixe N=3 | âš ï¸ Partiel (fuzzy, mais pas structure) |
| **pg_search** | Tantivy tokenizers | âœ… Via config | âš ï¸âš ï¸ Documentation limitÃ©e |
| **VectorChord** | pg_tokenizer (ultra-flexible) | âœ…âœ… Mappings, filters | âœ…âœ…âœ… Optimal (keep `_`, `/`, `.`) |
| **plpgsql_bm25** | `string_to_array()` | âš ï¸ Manuel SQL | âš ï¸ Basique (whitespace split) |

---

### Recommandation Tokenization

#### Phase 1-3 (pg_trgm)

**Tokenization**: Trigrams (3 chars), natif

**StratÃ©gie**:
1. âœ… Accepter limitations (fuzzy > structure)
2. âœ… Queries: encourager sÃ©mantique (embeddings) vs lexical
3. âš ï¸ Documentation: "pg_trgm coupe underscores, utiliser embeddings pour queries exactes"

---

#### v1.5.0+ (pg_search ou VectorChord)

**Tokenization**: Code-aware splitting

**Configuration recommandÃ©e** (pg_search):
```sql
-- Tokenizer code-aware (hypothÃ©tique, Ã  valider doc ParadeDB)
CREATE INDEX code_bm25_idx ON code_chunks
USING bm25 (id, source_code)
WITH (
    key_field='id',
    text_fields='{
        "source_code": {
            "tokenizer": {
                "type": "regex",
                "pattern": "[\\s(){}\\[\\];,]+",  -- Split whitespace + ponctuation SAUF _/.
                "lowercase": false  -- Preserve case (optionnel)
            }
        }
    }'
);
```

**Configuration VectorChord**:
```sql
SELECT create_text_analyzer('code_analyzer', $$
pre_tokenizer = "whitespace"
[[character_filters]]
type = "mapping"
mappings = []  -- Pas de remplacement
[[token_filters]]
type = "lowercase"  -- Optionnel: lowercase
$$);
```

**Validation requise**: Tester tokenizers rÃ©els sur corpus code (~1000 fonctions).

---

## ğŸ¯ Partie 5: Recommandations Finales Ultra-DÃ©taillÃ©es

### Recommandation 1: Migration PostgreSQL 18

#### âœ… APPROUVÃ‰: Migrer vers PostgreSQL 18

**Timeline**: **Phase 0 EPIC-06** (Infrastructure Setup, Semaine 1)

**Justification**:
1. **Skip Scan B-tree** (gain 30-50% queries multi-colonnes)
2. **GIN Parallel Creation** (gain 2-4Ã— indexing Phase 1)
3. **Async I/O** (gain 10-20% scans larges)
4. **Optimizer** (gains cumulatifs 5-15%)

**Total gains estimÃ©s**: 20-40% performance globale (indexing + queries)

**Risques**:
- âš ï¸ Extensions compatibility (pgvector, pg_search)
- âš ï¸ Bugs potentiels PostgreSQL 18 (nouveau)

**Mitigation**:
- âœ… VÃ©rifier compatibility extensions **AVANT** migration
- âœ… Tests regression complets (API, DB, performance)
- âœ… Backup DB avant migration

**Plan migration** (1-2 jours):
```bash
# Jour 1: PrÃ©paration
# 1. VÃ©rifier extensions
docker run --rm postgres:18-alpine psql --version
# Check pgvector: https://github.com/pgvector/pgvector/releases
# Check pg_search: https://github.com/paradedb/paradedb/releases

# 2. Update Dockerfile
# FROM postgres:17-alpine â†’ FROM postgres:18-alpine

# 3. Backup DB prod
make db-backup

# Jour 2: Migration + Tests
# 1. Build new image
docker-compose build

# 2. Test migration (DB test)
make db-test-reset
make api-test

# 3. Production migration
docker-compose down
docker-compose up -d
alembic upgrade head

# 4. Validation
make health
make benchmark
```

**Checklist validation**:
- [ ] pgvector 0.8.0+ compatible PostgreSQL 18 (vÃ©rifier releases)
- [ ] pg_trgm compatible (native, toujours OK)
- [ ] pg_search 0.19.0+ compatible PostgreSQL 18 (si adoption future)
- [ ] Tests regression API passent (142 tests)
- [ ] Benchmarks performance validÃ©s (latency â‰¤ baseline)
- [ ] Docker image build successful
- [ ] Backup DB prod crÃ©Ã©

**Go/No-Go**: âœ… GO si checklist 100%

---

### Recommandation 2: BM25 Search Strategy

#### âœ… APPROUVÃ‰: Approche Progressive (pg_trgm â†’ pg_search)

**Phase 1-3 (v1.4.0)**: **pg_trgm + pgvector + RRF**

**Justification**:
1. âœ… Mature, stable, natif PostgreSQL
2. âœ… ZÃ©ro nouvelle dÃ©pendance (respect contrainte)
3. âœ… Fuzzy matching = bonus
4. âœ… Suffisant MVP (Recall 65-75% acceptable)
5. âœ… Upgrade path clair (pg_search installation 5 min)

**ImplÃ©mentation**:
```sql
-- Phase 1: Setup pg_trgm
CREATE EXTENSION IF NOT EXISTS pg_trgm;
CREATE INDEX idx_code_source_trgm ON code_chunks USING gin (source_code gin_trgm_ops);
CREATE INDEX idx_code_name_trgm ON code_chunks USING gin (name gin_trgm_ops);

-- Phase 3: Hybrid Search Service
-- Lexical: pg_trgm
-- Semantic: pgvector HNSW
-- Fusion: RRF (k=60)
```

**MÃ©triques success**:
- Recall@10: >65% (target: 70%)
- Precision@10: >60% (target: 65%)
- Latency P95: <50ms (10k chunks)

---

**Post-Phase 3 Benchmark**: **Ã‰valuer qualitÃ©**

**DÃ©clencheur migration pg_search**: **Recall@10 < 80%**

**Rationale**:
- 80% = seuil acceptable code search production
- <80% = frustration users ("search misses obvious code")

**Benchmark dataset**:
- Codebase rÃ©elle (~500 functions Python)
- 50 queries manuelles (ex: "async iterator", "serialize JSON", "calculate hash")
- Mesures: Recall@10, Precision@10, MRR (Mean Reciprocal Rank)

---

**v1.5.0 (Post-Benchmark)**: **Migration pg_search SI requis**

**Justification**:
1. âœ… BM25 vrai (gain 10-15% Recall)
2. âœ… Installation .deb triviale (5 min)
3. âœ… Typo-tolerance intÃ©grÃ©
4. âš ï¸ MaturitÃ© acceptable (6 mois post-v1.4.0, ParadeDB plus stable)

**Timeline migration pg_search** (1-2 jours):
```bash
# Jour 1: Installation
curl -L "https://github.com/paradedb/paradedb/releases/download/v0.19.0/postgresql-18-pg-search_0.19.0-1PARADEDB-noble_amd64.deb" -o /tmp/pg_search.deb
sudo docker cp /tmp/pg_search.deb mnemo-postgres:/tmp/
sudo docker exec mnemo-postgres apt-get install -y /tmp/pg_search.deb

# Jour 2: Migration SQL + Code
# 1. Migration SQL
CREATE EXTENSION pg_search;
CREATE INDEX code_bm25_idx ON code_chunks USING bm25 (id, path, source_code, name) WITH (key_field='id');

# 2. Update HybridSearchService
# Replace pg_trgm query with pg_search @@@ operator

# 3. Tests + Benchmark
make api-test
make benchmark
```

**Gain attendu**: Recall@10: 70% â†’ 85% (+15%)

---

### Recommandation 3: Tokenization Strategy

#### âœ… APPROUVÃ‰: Tokenization Progressive

**Phase 1-3 (pg_trgm)**: **Trigrams (accept limitations)**

**Documentation utilisateur**:
> "Code search utilise pg_trgm (trigram similarity). Limitations:
> - Underscores ignorÃ©s: `calculate_total` â‰ˆ `calculatetotal`
> - Recommandation: Utiliser queries sÃ©mantiques (embeddings) pour matches exacts
> - Bonus: Fuzzy matching automatique (typos tolÃ©rÃ©es)"

---

**v1.5.0+ (pg_search)**: **Code-aware tokenization (Ã€ valider)**

**Plan validation tokenizers**:
1. CrÃ©er dataset test (100 code snippets)
2. Tester tokenizers pg_search:
   - `"default"` (whitespace + lowercase)
   - `"whitespace"` (whitespace uniquement)
   - `"regex"` (pattern custom `[\\s(){}]+` mais preserve `_`)
3. Mesurer impact:
   - Query `calculate_total` â†’ match exact?
   - Query `camelCase` â†’ split ou intact?
   - Paths `src/lib.rs` â†’ tokens?

**DÃ©cision finale**: BasÃ©e sur rÃ©sultats tests rÃ©els

---

**v1.6.0+ (VectorChord - Optionnel)**: **Tokenization ultra-configurable**

**SI**:
- Tokenization critique (queries exactes `snake_case` mandatoires)
- Performance absolue requise (Block-WeakAnd)
- Compilation Rust acceptable

**SINON**: Conserver pg_search (balance simplicitÃ©/performance)

---

### Recommandation 4: Hybrid Search Architecture

#### âœ… APPROUVÃ‰: RRF Fusion (k=60)

**Architecture finale** (Phase 3):
```
Query: "async iterator"
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Parallel Execution (3 searches)          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. Lexical (pg_trgm)                     â”‚
â”‚    â†’ Top 100 results (ranked by similarity)
â”‚                                           â”‚
â”‚ 2. Semantic (pgvector HNSW)              â”‚
â”‚    â†’ Top 100 results (ranked by <=> distance)
â”‚                                           â”‚
â”‚ 3. Graph (optional, depth 0-3)           â”‚
â”‚    â†’ Expand results with call graph      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ RRF Fusion (k=60)                         â”‚
â”‚ score(item) = Î£ 1/(rank_i + 60)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Top 20 results (hybrid ranked)
```

**ParamÃ¨tres optimaux**:
- k=60 (standard industry, Elasticsearch default)
- Weights: pg_trgm=1.0, pgvector=1.0 (Ã©quilibre)
- Graph expansion: optionnel (param `expand_graph=true`)

**SQL Pattern**:
```sql
WITH
lexical AS (
  SELECT id, row_number() OVER (ORDER BY similarity(source_code, $query) DESC) AS r
  FROM code_chunks
  WHERE source_code % $query
  LIMIT 100
),
semantic AS (
  SELECT id, row_number() OVER (ORDER BY embedding_code <=> $embedding) AS r
  FROM code_chunks
  ORDER BY embedding_code <=> $embedding
  LIMIT 100
)
SELECT c.id, c.name, c.path,
       1.0/(60 + COALESCE(lexical.r, 1e9)) + 1.0/(60 + COALESCE(semantic.r, 1e9)) AS rrf
FROM code_chunks c
LEFT JOIN lexical ON lexical.id = c.id
LEFT JOIN semantic ON semantic.id = c.id
ORDER BY rrf DESC
LIMIT 20;
```

---

## ğŸ“Š SynthÃ¨se Finale: Plan d'Action Complet

### Timeline Globale

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Phase 0: Infrastructure (Semaine 1)                            â”‚
â”‚ â€¢ PostgreSQL 17 â†’ 18 migration                                 â”‚
â”‚ â€¢ Validation extensions (pgvector, pg_trgm)                    â”‚
â”‚ â€¢ Alembic async setup                                          â”‚
â”‚ â€¢ Dual embeddings service                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Phase 1-3: pg_trgm + pgvector (Semaines 2-11)                 â”‚
â”‚ â€¢ Tree-sitter chunking                                         â”‚
â”‚ â€¢ pg_trgm indexes (GIN parallel, PostgreSQL 18 boost)         â”‚
â”‚ â€¢ Hybrid search (pg_trgm + vector + RRF)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Post-Phase 3: Benchmark (Semaine 12)                          â”‚
â”‚ â€¢ Dataset: 500 functions, 50 queries                          â”‚
â”‚ â€¢ Mesures: Recall@10, Precision@10, MRR                       â”‚
â”‚ â€¢ DÃ©clencheur: Recall@10 < 80% â†’ Migrer pg_search            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ v1.5.0 (SI Recall < 80%, ~6 mois post-v1.4.0)                â”‚
â”‚ â€¢ Migration pg_search (1-2 jours)                             â”‚
â”‚ â€¢ BM25 vrai + typo-tolerance                                  â”‚
â”‚ â€¢ Gain: Recall 70% â†’ 85% (+15%)                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ v1.6.0+ (Ã‰valuation future, optionnel)                        â”‚
â”‚ â€¢ VectorChord-BM25 (si tokenization critique)                 â”‚
â”‚ â€¢ Block-WeakAnd performance boost                             â”‚
â”‚ â€¢ Tokenization ultra-configurable                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Checklist DÃ©cisions

| DÃ©cision | Recommandation | Timeline | Statut |
|----------|----------------|----------|--------|
| **PostgreSQL 18 migration** | âœ… APPROUVÃ‰ | Phase 0 (Semaine 1) | â³ Ã€ valider extensions |
| **BM25 Phase 1-3** | âœ… pg_trgm (natif) | Phase 1-3 (Semaines 2-11) | âœ… VALIDÃ‰ |
| **BM25 Post-Benchmark** | âœ… pg_search SI Recall<80% | v1.5.0 (~6 mois) | â³ Conditionnel |
| **Tokenization Phase 1-3** | âœ… Trigrams (accept limits) | Phase 1-3 | âœ… VALIDÃ‰ |
| **Tokenization v1.5.0+** | âœ… Code-aware (Ã  valider) | v1.5.0 | â³ Tests requis |
| **Hybrid Search** | âœ… RRF (k=60) | Phase 3 (Semaine 9-11) | âœ… VALIDÃ‰ |

---

### Risques Majeurs & Mitigations

| Risque | ProbabilitÃ© | Impact | Mitigation |
|--------|-------------|--------|------------|
| **PostgreSQL 18 bugs** | Moyen | Haut | Tests regression complets, backup DB |
| **pgvector incompatible PG18** | Faible | Critique | Valider releases pgvector AVANT migration |
| **pg_trgm Recall < 60%** | Moyen | Moyen | Documenter limitations, encourager embeddings |
| **pg_search instable** | Moyen | Moyen | Attendre v1.5.0 (~6 mois), maturitÃ© amÃ©liorÃ©e |
| **Tokenization pg_search inadÃ©quate** | Moyen | Moyen | Tests validation dataset, fallback VectorChord |

---

## ğŸ¯ Conclusion

**PostgreSQL 18**: âœ… Migration recommandÃ©e Phase 0 (gains 20-40% performance)

**BM25 Strategy**: âœ… Approche progressive validÃ©e
1. Phase 1-3: pg_trgm (mature, stable, natif)
2. Post-Benchmark: pg_search SI Recall < 80%
3. v1.6.0+: VectorChord SI tokenization critique

**Tokenization**: âœ… Progressive (trigrams â†’ code-aware)

**Hybrid Search**: âœ… RRF (k=60) optimal

**Prochaines actions**:
1. âœ… Valider extensions compatibility PostgreSQL 18
2. âœ… Migrer PostgreSQL 18 (Phase 0, Semaine 1)
3. âœ… ImplÃ©menter pg_trgm hybrid search (Phase 3)
4. â³ Benchmark post-Phase 3 (dÃ©cision pg_search)

---

**Date**: 2025-10-15
**Version**: 1.0.0
**Statut**: âœ… RECHERCHE ULTRA-APPROFONDIE COMPLÃ‰TÃ‰E
**Validations**: Web research cross-checked, benchmarks estimÃ©s, plan d'action dÃ©taillÃ©

**Document maintenu par**: Ã‰quipe MnemoLite
