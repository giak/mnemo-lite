# EPIC-06: Deep Analysis - Code Intelligence + M√©moire Assistant

**Date**: 2025-10-15
**Statut**: üî¨ ANALYSE APPROFONDIE
**Context**: Double-check suite √† contrainte critique d√©couverte

---

## ‚ö†Ô∏è CONTRAINTE CRITIQUE IDENTIFI√âE

**MnemoLite doit CONTINUER √† √™tre une m√©moire pour assistant (conversations, documentation texte) TOUT EN AJOUTANT les capacit√©s code.**

**Probl√®me** :
- EPIC-06 initial se concentrait uniquement sur code intelligence
- Risque de casser le use case principal actuel (agent memory)
- Besoin d'architecture **dual-purpose** : texte g√©n√©ral + code

**Solution** : Architecture unifi√©e avec dual embeddings.

---

## üî¨ Ultra-Brainstorming: Analyse Comparative Embeddings

### Nomic Embed Code - Analyse D√©taill√©e

**Specs r√©elles** (recherche 2024-2025) :

| Propri√©t√© | Valeur | Source |
|-----------|--------|--------|
| **Param√®tres** | **7B** | Hugging Face (confirm√©) |
| **Architecture** | Qwen2-based | Nomic AI blog |
| **Dimensions** | 768D (probable, Qwen2 standard) | Inference |
| **Context Length** | 32,768 tokens | Hugging Face |
| **Langages** | Python, Java, Ruby, PHP, JS, Go | Hugging Face |
| **Performance** | State-of-the-art CodeSearchNet | Paper ICLR 2025 |
| **Licence** | Apache 2.0 | Hugging Face |
| **GGUF** | Oui (quantization disponible) | lmstudio-community |

**‚ö†Ô∏è PROBL√àME MAJEUR** :
```
nomic-embed-text-v1.5  : 137M params (~262 MB VRAM)
nomic-embed-code       : 7B params   (~13-14 GB VRAM full precision)

Ratio: 51√ó plus lourd!
```

**Implications** :
- CPU-only deployment difficile (7B params)
- Latence plus √©lev√©e
- RAM requirements significatifs (>4GB)
- Pas id√©al pour d√©ploiement local l√©ger

---

### ü•á MEILLEURE ALTERNATIVE : Jina Code Embeddings

#### Option 1: jina-embeddings-v2-base-code (RECOMMAND√â)

| Propri√©t√© | Valeur | Avantage MnemoLite |
|-----------|--------|-------------------|
| **Param√®tres** | 161M | 43√ó plus l√©ger que nomic-code |
| **Architecture** | BERT-based | √âprouv√©, stable |
| **Dimensions** | **768D** | ‚úÖ IDENTIQUE nomic-text! |
| **Context Length** | 8192 tokens | Suffisant pour fonctions |
| **Langages** | 30+ programming languages | Tr√®s complet |
| **Performance** | Lead 9/15 CodeSearchNet benchmarks | Tr√®s bon |
| **Licence** | Apache 2.0 | Compatible |
| **RAM** | ~300-400 MB | L√©ger |
| **Latence** | <50ms/batch | Rapide |

**üéØ PERFECT FIT** :
- **M√™me dimensionnalit√© (768D)** que nomic-text ‚Üí pas de migration DB!
- L√©ger (161M) ‚Üí d√©ploiement facile
- Performance excellente (mieux que Microsoft/Salesforce)
- Multi-langages (30+)

#### Option 2: jina-code-embeddings-1.5b (SOTA 2025)

| Propri√©t√© | Valeur | Trade-off |
|-----------|--------|-----------|
| **Param√®tres** | 1.5B | 10√ó plus l√©ger que nomic-code, mais 10√ó plus lourd que jina-v2 |
| **Dimensions** | 1536D (d√©faut) | ‚ö†Ô∏è Incompatible 768D actuel |
| **Matryoshka Truncation** | Oui (768D possible) | ‚úÖ Performance loss minimal |
| **Performance** | 79.04% avg (25 benchmarks) | State-of-the-art 2025 |
| **CodeSearchNet** | 86.45% | Excellent |
| **Context Length** | 8192 tokens | OK |

**Trade-offs** :
- ‚úÖ Meilleure performance 2025 (SOTA)
- ‚úÖ Match voyage-code-3 (79.23%)
- ‚úÖ Truncation 768D possible via Matryoshka
- ‚ö†Ô∏è Plus lourd que jina-v2 (1.5B vs 161M)
- ‚ö†Ô∏è Needs quantization pour CPU l√©ger

#### Option 3: jina-code-embeddings-0.5b (Ultra-l√©ger)

| Propri√©t√© | Valeur | Trade-off |
|-----------|--------|-----------|
| **Param√®tres** | 500M | 14√ó plus l√©ger que nomic-code |
| **Dimensions** | 896D (d√©faut) | ‚ö†Ô∏è Incompatible 768D |
| **Matryoshka Truncation** | Oui (64D minimum) | ‚úÖ 768D possible |
| **Performance** | 78.41% avg | Tr√®s bon pour la taille |
| **Outperforms** | Qwen3-0.6B (-20% params) | Efficient |

**Trade-offs** :
- ‚úÖ Ultra-l√©ger (500M)
- ‚úÖ Performance correcte
- ‚ö†Ô∏è Moins bon que 1.5B version
- ‚ö†Ô∏è Truncation 768D n√©cessaire

---

### üìä Benchmark Comparatif Final

| Mod√®le | Params | Dims | CodeSearchNet | RAM | Latence | Compatibilit√© 768D | Verdict |
|--------|--------|------|---------------|-----|---------|-------------------|---------|
| **nomic-embed-code** | 7B | 768 | SOTA | ~14 GB | Lent | ‚úÖ Natif | ‚ùå Trop lourd |
| **jina-code-1.5B** | 1.5B | 1536‚Üí768 | 86.45% | ~3 GB | Moyen | ‚úÖ Truncation | ‚≠ê SOTA 2025 |
| **jina-v2-base-code** | 161M | 768 | ~75%* | ~400 MB | Rapide | ‚úÖ Natif | ü•á RECOMMAND√â |
| **jina-code-0.5B** | 500M | 896‚Üí768 | 78.41% | ~1 GB | Rapide | ‚úÖ Truncation | ‚ö° Ultra-l√©ger |

*Estimation bas√©e sur "lead 9/15 benchmarks"

---

## üèóÔ∏è Architecture Unifi√©e: Dual-Purpose Memory

### Use Cases √† Supporter

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ          MnemoLite v1.4.0 Use Cases                 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                     ‚îÇ
‚îÇ  1. AGENT MEMORY (Actuel - √Ä PR√âSERVER)            ‚îÇ
‚îÇ     ‚Ä¢ Conversations avec Claude/GPT                 ‚îÇ
‚îÇ     ‚Ä¢ Documentation technique (ADRs, RFCs)          ‚îÇ
‚îÇ     ‚Ä¢ Notes de d√©veloppement                        ‚îÇ
‚îÇ     ‚Ä¢ D√©cisions architecturales                     ‚îÇ
‚îÇ     ‚Üí Embeddings: TEXTE G√âN√âRAL                     ‚îÇ
‚îÇ                                                     ‚îÇ
‚îÇ  2. CODE INTELLIGENCE (Nouveau - √Ä AJOUTER)         ‚îÇ
‚îÇ     ‚Ä¢ Indexation codebase                           ‚îÇ
‚îÇ     ‚Ä¢ Recherche s√©mantique de code                  ‚îÇ
‚îÇ     ‚Ä¢ Navigation call graph                         ‚îÇ
‚îÇ     ‚Ä¢ Documentation code inline                     ‚îÇ
‚îÇ     ‚Üí Embeddings: CODE SP√âCIALIS√â                   ‚îÇ
‚îÇ                                                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Strat√©gie: Dual Embeddings, Unified Schema

**Architecture propos√©e** :

```sql
-- Option A: Extension table events (backward compatible)
ALTER TABLE events ADD COLUMN embedding_code VECTOR(768);
ALTER TABLE events ADD COLUMN content_type TEXT DEFAULT 'text';
-- content_type: 'text', 'code', 'conversation', 'documentation'

-- Option B: Table d√©di√©e code (s√©paration propre)
CREATE TABLE code_chunks (
    id UUID PRIMARY KEY,
    source_code TEXT NOT NULL,
    language TEXT NOT NULL,
    chunk_type TEXT NOT NULL,
    embedding_text VECTOR(768),   -- Embedding texte g√©n√©ral (pour docstrings, comments)
    embedding_code VECTOR(768),   -- Embedding code sp√©cialis√©
    metadata JSONB NOT NULL,
    ...
);

-- Option C: Unified table avec dual vectors (flexible)
CREATE TABLE memory_items (
    id UUID PRIMARY KEY,
    content TEXT NOT NULL,
    item_type TEXT NOT NULL,  -- 'event', 'code_chunk', 'conversation', 'document'
    embedding_text VECTOR(768),  -- Always populated (texte g√©n√©ral)
    embedding_code VECTOR(768),  -- Populated only if item_type='code_chunk'
    metadata JSONB NOT NULL,
    indexed_at TIMESTAMPTZ DEFAULT NOW()
);

-- Index HNSW pour les deux embeddings
CREATE INDEX idx_memory_embedding_text ON memory_items
USING hnsw (embedding_text vector_cosine_ops) WITH (m = 16, ef_construction = 64);

CREATE INDEX idx_memory_embedding_code ON memory_items
USING hnsw (embedding_code vector_cosine_ops) WITH (m = 16, ef_construction = 64)
WHERE embedding_code IS NOT NULL;  -- Partial index (code only)
```

---

## üéØ RECOMMANDATION FINALE

### Strat√©gie Recommand√©e: "Dual Lightweight"

**Embeddings Models** :

1. **Texte/Conversations** (existant) :
   - **nomic-embed-text-v1.5**
   - 137M params, 768D
   - ‚úÖ D√©j√† en production
   - ‚úÖ Performant sur texte g√©n√©ral
   - ‚úÖ L√©ger, rapide

2. **Code** (nouveau) :
   - **jina-embeddings-v2-base-code**
   - 161M params, 768D
   - ‚úÖ M√™me dimensionnalit√© (pas de migration!)
   - ‚úÖ L√©ger (300M total avec nomic-text)
   - ‚úÖ Excellent performance (lead 9/15 benchmarks)
   - ‚úÖ 30+ langages
   - ‚úÖ D√©ploiement facile (sentence-transformers compatible)

**Total RAM** : ~300M + ~400M = **~700 MB** (vs 14 GB nomic-code!)

---

### Architecture DB: Option B (Tables S√©par√©es)

**Justification** :
- ‚úÖ S√©paration claire use cases (events vs code)
- ‚úÖ Schemas optimis√©s par type
- ‚úÖ Backward compatibility totale (table events intacte)
- ‚úÖ Dual embeddings sur code_chunks seulement (√©conomie m√©moire)
- ‚úÖ √âvolutivit√© (facile ajouter autres types: images, audio, etc.)

**Schema final** :

```sql
-- Table 1: events (INCHANG√âE - agent memory)
CREATE TABLE events (
    id UUID PRIMARY KEY,
    timestamp TIMESTAMPTZ NOT NULL,
    content JSONB NOT NULL,
    embedding VECTOR(768),  -- nomic-embed-text-v1.5
    metadata JSONB
);

-- Table 2: code_chunks (NOUVELLE - code intelligence)
CREATE TABLE code_chunks (
    id UUID PRIMARY KEY,
    file_path TEXT NOT NULL,
    language TEXT NOT NULL,
    chunk_type TEXT NOT NULL,
    name TEXT,
    source_code TEXT NOT NULL,
    start_line INT,
    end_line INT,

    -- Dual embeddings
    embedding_text VECTOR(768),  -- nomic-text (docstrings, comments)
    embedding_code VECTOR(768),  -- jina-code (code s√©mantique)

    metadata JSONB NOT NULL,
    indexed_at TIMESTAMPTZ DEFAULT NOW(),
    node_id UUID,
    repository TEXT
);

-- Index HNSW distincts
CREATE INDEX idx_events_embedding ON events
USING hnsw (embedding vector_cosine_ops);

CREATE INDEX idx_code_embedding_text ON code_chunks
USING hnsw (embedding_text vector_cosine_ops);

CREATE INDEX idx_code_embedding_code ON code_chunks
USING hnsw (embedding_code vector_cosine_ops);
```

---

### EmbeddingService: Dual Model Support

```python
# api/services/embedding_service.py
from sentence_transformers import SentenceTransformer
from enum import Enum

class EmbeddingDomain(str, Enum):
    TEXT = "text"        # Conversations, docs, general text
    CODE = "code"        # Code snippets, functions, classes
    HYBRID = "hybrid"    # Generate both (for code with docstrings)

class EmbeddingService:
    def __init__(self):
        # Model 1: General text (existing)
        self.text_model = SentenceTransformer(settings.EMBEDDING_MODEL)
        # "nomic-ai/nomic-embed-text-v1.5"

        # Model 2: Code specialized (new)
        self.code_model = SentenceTransformer(settings.CODE_EMBEDDING_MODEL)
        # "jinaai/jina-embeddings-v2-base-code"

    async def generate_embedding(
        self,
        text: str,
        domain: EmbeddingDomain = EmbeddingDomain.TEXT
    ) -> dict[str, list[float]]:
        """
        Generate embedding(s) based on domain.

        Args:
            text: Text or code to embed
            domain: TEXT, CODE, or HYBRID

        Returns:
            {'text': [...], 'code': [...]} or {'text': [...]} or {'code': [...]}
        """
        result = {}

        if domain in [EmbeddingDomain.TEXT, EmbeddingDomain.HYBRID]:
            emb_text = self.text_model.encode(text, convert_to_tensor=False)
            result['text'] = emb_text.tolist()

        if domain in [EmbeddingDomain.CODE, EmbeddingDomain.HYBRID]:
            emb_code = self.code_model.encode(text, convert_to_tensor=False)
            result['code'] = emb_code.tolist()

        return result

    def auto_detect_domain(self, content_type: str) -> EmbeddingDomain:
        """
        Auto-detect embedding domain from content type.

        Args:
            content_type: 'event', 'code_chunk', 'conversation', 'documentation'

        Returns:
            EmbeddingDomain
        """
        if content_type == 'code_chunk':
            return EmbeddingDomain.HYBRID  # Code + docstrings
        else:
            return EmbeddingDomain.TEXT  # General text
```

---

### Search Service: Unified Hybrid Search

```python
# api/services/unified_search_service.py
class UnifiedSearchService:
    async def search(
        self,
        query: str,
        search_in: list[str] = ['events', 'code'],
        use_bm25: bool = True,
        use_vector: bool = True,
        limit: int = 10
    ) -> SearchResults:
        """
        Unified search across events (agent memory) and code.

        Args:
            query: Search query
            search_in: ['events'], ['code'], or ['events', 'code']
            use_bm25: Enable BM25 lexical search
            use_vector: Enable vector semantic search
            limit: Max results

        Returns:
            Unified SearchResults
        """
        results = []

        # Search in events (agent memory)
        if 'events' in search_in:
            event_results = await self._search_events(
                query, use_bm25, use_vector
            )
            results.extend(event_results)

        # Search in code
        if 'code' in search_in:
            code_results = await self._search_code(
                query, use_bm25, use_vector
            )
            results.extend(code_results)

        # Merge and rank (RRF if multiple sources)
        if len(results) > 0:
            ranked = self._rank_unified_results(results)
            return SearchResults(
                data=ranked[:limit],
                meta={
                    'sources': search_in,
                    'total': len(ranked)
                }
            )

        return SearchResults(data=[], meta={'total': 0})

    async def _search_events(self, query, use_bm25, use_vector):
        """Search in events table (agent memory)."""
        # Generate text embedding
        query_emb = await self.embedding_service.generate_embedding(
            query, domain=EmbeddingDomain.TEXT
        )

        # Vector search
        if use_vector:
            results = await self.event_repo.vector_search(
                embedding=query_emb['text'],
                limit=100
            )

        # BM25 search (if enabled)
        if use_bm25:
            bm25_results = await self.event_repo.bm25_search(query, limit=100)
            # Merge with RRF...

        return results

    async def _search_code(self, query, use_bm25, use_vector):
        """Search in code_chunks table."""
        # Generate code embedding
        query_emb = await self.embedding_service.generate_embedding(
            query, domain=EmbeddingDomain.CODE
        )

        # Vector search on embedding_code
        if use_vector:
            results = await self.code_chunk_repo.vector_search(
                embedding=query_emb['code'],
                embedding_field='embedding_code',  # Specify which vector
                limit=100
            )

        # BM25 search on source_code
        if use_bm25:
            bm25_results = await self.code_chunk_repo.bm25_search(query, limit=100)
            # Merge with RRF...

        return results
```

---

## üìä Comparaison Options Architecture

### Option A: Extension Table Events

```sql
ALTER TABLE events ADD COLUMN embedding_code VECTOR(768);
ALTER TABLE events ADD COLUMN content_type TEXT;
```

**Avantages** :
- ‚úÖ Simple (une seule table)
- ‚úÖ Backward compatible

**Inconv√©nients** :
- ‚ùå Mixing concerns (events ‚â† code chunks)
- ‚ùå Schema rigide (doit fitter les deux use cases)
- ‚ùå embedding_code NULL pour 99% des events (waste)
- ‚ùå Pas de metadata sp√©cifique code (complexity, signature, etc.)

**Verdict** : ‚ùå Pas recommand√© (mixing incompatible use cases)

---

### Option B: Tables S√©par√©es ‚≠ê RECOMMAND√â

```sql
-- events: agent memory (inchang√©)
-- code_chunks: code intelligence (nouveau)
```

**Avantages** :
- ‚úÖ S√©paration claire des concerns
- ‚úÖ Schemas optimis√©s par use case
- ‚úÖ Backward compatibility totale
- ‚úÖ Dual embeddings sur code seulement (√©conomie m√©moire)
- ‚úÖ Metadata sp√©cifique par type
- ‚úÖ Index HNSW optimis√©s s√©par√©ment
- ‚úÖ √âvolutivit√© (facile ajouter autres types: images, audio)

**Inconv√©nients** :
- ‚ö†Ô∏è Recherche unifi√©e = 2 requ√™tes + merge (acceptable)
- ‚ö†Ô∏è L√©g√®re complexit√© service layer

**Verdict** : ü•á **RECOMMAND√â** (best practices, √©volutif)

---

### Option C: Table Unifi√©e memory_items

```sql
CREATE TABLE memory_items (
    item_type TEXT,  -- 'event', 'code_chunk', 'conversation', ...
    embedding_text VECTOR(768),
    embedding_code VECTOR(768),
    ...
);
```

**Avantages** :
- ‚úÖ Flexible (supporte futurs types: images, audio)
- ‚úÖ Recherche unifi√©e simple (une table)
- ‚úÖ Dual embeddings natifs

**Inconv√©nients** :
- ‚ùå Migration lourde (rewrite table events)
- ‚ùå Breaking change majeur
- ‚ùå Schema generic = perte de sp√©cificit√©
- ‚ùå embedding_code NULL pour events (waste)
- ‚ùå Complexit√© metadata (JSONB tr√®s h√©t√©rog√®ne)

**Verdict** : ‚ö†Ô∏è Trop disruptif pour v1.4.0, consid√©rer v2.0.0

---

## üéØ Plan d'Impl√©mentation R√©vis√©

### Phase 0: Pr√©paration (1 semaine)

**Objectif** : Setup dual embeddings sans casser existant

1. **Setup jina-embeddings-v2-base-code**
   ```bash
   pip install sentence-transformers
   # Auto-download jinaai/jina-embeddings-v2-base-code
   ```

2. **Extend EmbeddingService**
   - Ajouter `self.code_model`
   - M√©thode `generate_embedding(text, domain='text'|'code'|'hybrid')`
   - Tests unitaires dual embeddings

3. **Benchmark local**
   - Comparer latence nomic-text vs jina-code
   - V√©rifier RAM usage (<1GB total)
   - Valider dimensions 768D identiques

**Livrable** : Dual embeddings op√©rationnel, test√©, backward compatible

---

### Phase 1: Foundation Code (4 semaines) - R√âVIS√â

**Stories inchang√©es** :
- ‚úÖ Story 1: Tree-sitter Chunking (13 pts)
- ‚úÖ Story 3: Metadata Extraction (8 pts)

**Story 2 R√âVIS√âE** : Dual Embeddings Setup (5 pts) ‚Üê R√âDUIT
- Setup jina-embeddings-v2-base-code (pas nomic-code)
- Dual model dans EmbeddingService
- Tests benchmark local
- Documentation migration

**Story 2bis NOUVELLE** : Code Chunks Table & Repo (5 pts)
- Cr√©ation table `code_chunks`
- Repository pattern `CodeChunkRepository`
- CRUD operations
- Tests int√©gration PostgreSQL

**Livrable Phase 1** :
- Chunking s√©mantique ‚úÖ
- Dual embeddings (text + code) ‚úÖ
- Metadata extraction ‚úÖ
- Table code_chunks cr√©√©e ‚úÖ

---

### Phase 2-4: Inchang√©es

- Phase 2: Graph Intelligence (3 semaines)
- Phase 3: Hybrid Search (3 semaines)
- Phase 4: Integration (2 semaines)

**Total** : 12 semaines (identique)

---

## üìä M√©triques de Succ√®s R√âVIS√âES

### Performance

| M√©trique | Baseline | Target | Strat√©gie |
|----------|----------|--------|-----------|
| **Events search (texte)** | 12ms | <15ms | Inchang√© (nomic-text) |
| **Code search (s√©mantique)** | N/A | <30ms | jina-code (l√©ger) |
| **Unified search (both)** | N/A | <50ms | Parallel queries + merge |
| **RAM usage** | 262 MB | <1 GB | ~700 MB (nomic-text + jina-code) |
| **Latence embedding gen** | 20-50ms | <100ms | Dual lightweight models |

### Quality

| M√©trique | Target | Mesure |
|----------|--------|--------|
| **Events recall** | >90% | Maintain actuel |
| **Code precision** | >80% | jina-v2 benchmarks |
| **Unified relevance** | >85% | RRF fusion |

---

## üöß Risques R√©vis√©s

| Risque | Probabilit√© | Impact | Mitigation R√âVIS√âE |
|--------|-------------|--------|-------------------|
| **Dual embeddings trop lents** | Faible | Moyen | jina-v2 l√©ger (161M), batch processing |
| **RAM overflow CPU** | Faible | Moyen | Total <1GB (700MB), quantization possible |
| **Unified search complexe** | Moyen | Moyen | Tables s√©par√©es = queries simples |
| **Breaking changes** | Faible | Haut | Table events intacte, code_chunks nouvelle |
| **Maintenance dual models** | Moyen | Faible | sentence-transformers handles updates |

---

## ‚úÖ Validation Finale

### Checklist Contraintes MnemoLite

- ‚úÖ **PostgreSQL 17 Only** : Tables s√©par√©es, indexes HNSW natifs
- ‚úÖ **Local deployment** : jina-v2 + nomic-text = 300M+400M, CPU-friendly
- ‚úÖ **Async-first** : SQLAlchemy 2.0 async, compatible architecture
- ‚úÖ **Backward compatible** : Table events intacte, API v1 inchang√©e
- ‚úÖ **768D embeddings** : jina-v2 natif 768D, pas de migration DB
- ‚úÖ **L√©ger** : Total ~700 MB RAM (vs 14 GB nomic-code)
- ‚úÖ **M√©moire assistant** : Events table pr√©serv√©e, use case intact
- ‚úÖ **Code intelligence** : code_chunks table ajout√©e, dual embeddings

### Checklist Use Cases

- ‚úÖ **Agent conversations** : Events table, nomic-text embeddings
- ‚úÖ **Documentation technique** : Events table, texte g√©n√©ral
- ‚úÖ **Code indexing** : code_chunks table, jina-code embeddings
- ‚úÖ **Code search** : Hybrid search sur embedding_code
- ‚úÖ **Unified search** : Merge events + code_chunks via RRF
- ‚úÖ **Graph navigation** : nodes/edges pour code dependencies

---

## üéâ Conclusion

### Recommandation Finale: Option B + jina-v2-base-code

**Architecture** :
- ‚úÖ Tables s√©par√©es (`events` + `code_chunks`)
- ‚úÖ Dual embeddings (nomic-text 137M + jina-code 161M)
- ‚úÖ Unified search service (merge r√©sultats)
- ‚úÖ 768D partout (pas de migration DB)

**Avantages** :
- ‚úÖ **L√©ger** : ~700 MB total (vs 14 GB nomic-code)
- ‚úÖ **Performant** : jina-v2 excellent (lead 9/15 benchmarks)
- ‚úÖ **Compatible** : 768D, backward compatible
- ‚úÖ **√âvolutif** : Architecture dual-purpose claire
- ‚úÖ **Pr√©serve use case actuel** : Agent memory intact

**Trade-off accept√©** :
- ‚ö†Ô∏è Pas le SOTA 2025 absolu (jina-code-1.5B meilleur)
- ‚úÖ Mais performance excellente + d√©ploiement facile
- ‚úÖ Ratio performance/poids optimal pour MnemoLite

**Alternative future (v1.5.0)** :
- Upgrade vers jina-code-embeddings-1.5B avec truncation Matryoshka 768D
- Si besoin SOTA absolu + si d√©ploiement plus puissant (GPU)

---

**Prochaine Action** :
- Mise √† jour EPIC-06.md avec strat√©gie dual embeddings
- Mise √† jour STORIES_EPIC-06.md Story 2 (remplacer nomic-code par jina-v2)
- Validation stakeholders avant Phase 0 kickoff

---

**Date**: 2025-10-15
**Statut**: ‚úÖ ANALYSE COMPL√àTE
**D√©cision**: **jina-embeddings-v2-base-code** (161M, 768D) + architecture tables s√©par√©es
